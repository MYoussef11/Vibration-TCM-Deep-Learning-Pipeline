2025-11-10 23:29:25,840:WARNING:<stdin>:1: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.

2025-11-10 23:29:29,542:WARNING:/mnt/c/Users/maraw/.vscode/Vibration-TCM-Deep-Learning-Pipeline/scripts/run_pycaret_baseline.py:11: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  from pycaret.classification import compare_models, finalize_model, predict_model, pull, save_model, setup

2025-11-10 23:29:31,283:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:29:31,283:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:29:31,284:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:29:31,284:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:29:45,497:WARNING:/mnt/c/Users/maraw/.vscode/Vibration-TCM-Deep-Learning-Pipeline/scripts/run_pycaret_baseline.py:11: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  from pycaret.classification import compare_models, finalize_model, predict_model, pull, save_model, setup

2025-11-10 23:29:46,309:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:29:46,309:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:29:46,309:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:29:46,309:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:29:47,057:INFO:PyCaret ClassificationExperiment
2025-11-10 23:29:47,057:INFO:Logging name: clf-default-name
2025-11-10 23:29:47,057:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-11-10 23:29:47,057:INFO:version 3.3.2
2025-11-10 23:29:47,058:INFO:Initializing setup()
2025-11-10 23:29:47,058:INFO:self.USI: 4dff
2025-11-10 23:29:47,058:INFO:self._variable_keys: {'X_train', 'gpu_n_jobs_param', 'fold_groups_param', 'n_jobs_param', 'y', '_available_plots', 'idx', 'fix_imbalance', '_ml_usecase', 'gpu_param', 'memory', 'pipeline', 'exp_id', 'is_multiclass', 'y_train', 'target_param', 'data', 'fold_shuffle_param', 'logging_param', 'X', 'y_test', 'html_param', 'fold_generator', 'seed', 'X_test', 'exp_name_log', 'log_plots_param', 'USI'}
2025-11-10 23:29:47,058:INFO:Checking environment
2025-11-10 23:29:47,059:INFO:python_version: 3.12.3
2025-11-10 23:29:47,059:INFO:python_build: ('main', 'Jun 18 2025 17:59:45')
2025-11-10 23:29:47,059:INFO:machine: x86_64
2025-11-10 23:29:47,061:INFO:platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
2025-11-10 23:29:47,061:INFO:Memory: svmem(total=3835613184, available=3065643008, percent=20.1, used=769970176, free=2644336640, active=189558784, inactive=651599872, buffers=32989184, cached=555868160, shared=3301376, slab=101650432)
2025-11-10 23:29:47,062:INFO:Physical Core: 6
2025-11-10 23:29:47,062:INFO:Logical Core: 12
2025-11-10 23:29:47,062:INFO:Checking libraries
2025-11-10 23:29:47,063:INFO:System:
2025-11-10 23:29:47,063:INFO:    python: 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]
2025-11-10 23:29:47,063:INFO:executable: /usr/bin/python3
2025-11-10 23:29:47,063:INFO:   machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
2025-11-10 23:29:47,063:INFO:PyCaret required dependencies:
2025-11-10 23:46:59,099:WARNING:/mnt/c/Users/maraw/.vscode/Vibration-TCM-Deep-Learning-Pipeline/scripts/run_pycaret_baseline.py:11: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  from pycaret.classification import compare_models, finalize_model, predict_model, pull, save_model, setup

2025-11-10 23:47:00,660:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:47:00,661:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:47:00,661:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:47:00,661:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:47:01,998:INFO:PyCaret ClassificationExperiment
2025-11-10 23:47:01,998:INFO:Logging name: clf-default-name
2025-11-10 23:47:01,998:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-11-10 23:47:01,998:INFO:version 3.3.2
2025-11-10 23:47:01,998:INFO:Initializing setup()
2025-11-10 23:47:01,999:INFO:self.USI: bd83
2025-11-10 23:47:01,999:INFO:self._variable_keys: {'log_plots_param', 'exp_name_log', '_ml_usecase', 'exp_id', 'gpu_n_jobs_param', 'target_param', 'idx', '_available_plots', 'fold_groups_param', 'data', 'X', 'logging_param', 'seed', 'X_test', 'n_jobs_param', 'fold_generator', 'html_param', 'y', 'X_train', 'USI', 'gpu_param', 'y_train', 'fix_imbalance', 'memory', 'pipeline', 'is_multiclass', 'fold_shuffle_param', 'y_test'}
2025-11-10 23:47:01,999:INFO:Checking environment
2025-11-10 23:47:01,999:INFO:python_version: 3.12.3
2025-11-10 23:47:01,999:INFO:python_build: ('main', 'Jun 18 2025 17:59:45')
2025-11-10 23:47:02,000:INFO:machine: x86_64
2025-11-10 23:47:02,002:INFO:platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
2025-11-10 23:47:02,002:INFO:Memory: svmem(total=3835613184, available=3063586816, percent=20.1, used=772026368, free=2900443136, active=73986048, inactive=513875968, buffers=15069184, cached=314834944, shared=3301376, slab=99745792)
2025-11-10 23:47:02,003:INFO:Physical Core: 6
2025-11-10 23:47:02,004:INFO:Logical Core: 12
2025-11-10 23:47:02,004:INFO:Checking libraries
2025-11-10 23:47:02,004:INFO:System:
2025-11-10 23:47:02,004:INFO:    python: 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]
2025-11-10 23:47:02,005:INFO:executable: /usr/bin/python3
2025-11-10 23:47:02,005:INFO:   machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
2025-11-10 23:47:02,005:INFO:PyCaret required dependencies:
2025-11-10 23:47:02,011:INFO:                 pip: 25.3
2025-11-10 23:47:02,011:INFO:          setuptools: 68.1.2
2025-11-10 23:47:02,012:INFO:             pycaret: 3.3.2
2025-11-10 23:47:02,012:INFO:             IPython: 9.7.0
2025-11-10 23:47:02,012:INFO:          ipywidgets: 8.1.8
2025-11-10 23:47:02,012:INFO:                tqdm: 4.67.1
2025-11-10 23:47:02,012:INFO:               numpy: 1.26.4
2025-11-10 23:47:02,013:INFO:              pandas: 2.1.4
2025-11-10 23:47:02,013:INFO:              jinja2: 3.1.2
2025-11-10 23:47:02,013:INFO:               scipy: 1.11.4
2025-11-10 23:47:02,013:INFO:              joblib: 1.3.2
2025-11-10 23:47:02,013:INFO:             sklearn: 1.4.2
2025-11-10 23:47:02,013:INFO:                pyod: 2.0.5
2025-11-10 23:47:02,014:INFO:            imblearn: 0.14.0
2025-11-10 23:47:02,014:INFO:   category_encoders: 2.7.0
2025-11-10 23:47:02,014:INFO:               numba: 0.62.1
2025-11-10 23:47:02,014:INFO:            requests: 2.31.0
2025-11-10 23:47:02,014:INFO:          matplotlib: 3.7.5
2025-11-10 23:47:02,014:INFO:          scikitplot: 0.3.7
2025-11-10 23:47:02,015:INFO:         yellowbrick: 1.5
2025-11-10 23:47:02,015:INFO:              plotly: 6.4.0
2025-11-10 23:47:02,015:INFO:    plotly-resampler: Not installed
2025-11-10 23:47:02,015:INFO:             kaleido: 1.2.0
2025-11-10 23:47:02,015:INFO:           schemdraw: 0.15
2025-11-10 23:47:02,015:INFO:         statsmodels: 0.14.5
2025-11-10 23:47:02,015:INFO:              sktime: 0.26.0
2025-11-10 23:47:02,016:INFO:               tbats: 1.1.3
2025-11-10 23:47:02,016:INFO:            pmdarima: 2.0.4
2025-11-10 23:47:02,016:INFO:              psutil: 7.1.3
2025-11-10 23:47:02,016:INFO:          markupsafe: 2.1.5
2025-11-10 23:47:02,016:INFO:             pickle5: Not installed
2025-11-10 23:47:02,016:INFO:         cloudpickle: 3.1.2
2025-11-10 23:47:02,017:INFO:         deprecation: 2.1.0
2025-11-10 23:47:02,017:INFO:              xxhash: 3.6.0
2025-11-10 23:47:02,017:INFO:           wurlitzer: 3.1.1
2025-11-10 23:47:02,017:INFO:PyCaret optional dependencies:
2025-11-10 23:47:02,154:INFO:                shap: Not installed
2025-11-10 23:47:02,154:INFO:           interpret: Not installed
2025-11-10 23:47:02,155:INFO:                umap: 0.5.9.post2
2025-11-10 23:47:02,155:INFO:     ydata_profiling: Not installed
2025-11-10 23:47:02,155:INFO:  explainerdashboard: Not installed
2025-11-10 23:47:02,155:INFO:             autoviz: Not installed
2025-11-10 23:47:02,155:INFO:           fairlearn: Not installed
2025-11-10 23:47:02,155:INFO:          deepchecks: Not installed
2025-11-10 23:47:02,155:INFO:             xgboost: 3.1.1
2025-11-10 23:47:02,156:INFO:            catboost: Not installed
2025-11-10 23:47:02,156:INFO:              kmodes: Not installed
2025-11-10 23:47:02,156:INFO:             mlxtend: Not installed
2025-11-10 23:47:02,156:INFO:       statsforecast: Not installed
2025-11-10 23:47:02,156:INFO:        tune_sklearn: Not installed
2025-11-10 23:47:02,156:INFO:                 ray: Not installed
2025-11-10 23:47:02,156:INFO:            hyperopt: Not installed
2025-11-10 23:47:02,157:INFO:              optuna: Not installed
2025-11-10 23:47:02,157:INFO:               skopt: Not installed
2025-11-10 23:47:02,157:INFO:              mlflow: Not installed
2025-11-10 23:47:02,157:INFO:              gradio: Not installed
2025-11-10 23:47:02,157:INFO:             fastapi: Not installed
2025-11-10 23:47:02,157:INFO:             uvicorn: Not installed
2025-11-10 23:47:02,157:INFO:              m2cgen: Not installed
2025-11-10 23:47:02,157:INFO:           evidently: Not installed
2025-11-10 23:47:02,158:INFO:               fugue: Not installed
2025-11-10 23:47:02,158:INFO:           streamlit: Not installed
2025-11-10 23:47:02,158:INFO:             prophet: Not installed
2025-11-10 23:47:02,158:INFO:None
2025-11-10 23:47:02,158:INFO:Set up data.
2025-11-10 23:47:02,174:INFO:Set up folding strategy.
2025-11-10 23:47:02,174:INFO:Set up train/test split.
2025-11-10 23:47:02,184:INFO:Set up index.
2025-11-10 23:47:02,184:INFO:Assigning column types.
2025-11-10 23:47:02,194:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-11-10 23:47:02,225:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-11-10 23:47:02,228:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-11-10 23:47:02,253:INFO:Soft dependency imported: xgboost: 3.1.1
2025-11-10 23:48:43,765:WARNING:/mnt/c/Users/maraw/.vscode/Vibration-TCM-Deep-Learning-Pipeline/scripts/run_pycaret_baseline.py:11: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  from pycaret.classification import compare_models, finalize_model, predict_model, pull, save_model, setup

2025-11-10 23:48:45,604:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:48:45,604:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:48:45,605:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:48:45,605:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-10 23:48:47,002:INFO:PyCaret ClassificationExperiment
2025-11-10 23:48:47,003:INFO:Logging name: clf-default-name
2025-11-10 23:48:47,003:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-11-10 23:48:47,003:INFO:version 3.3.2
2025-11-10 23:48:47,003:INFO:Initializing setup()
2025-11-10 23:48:47,003:INFO:self.USI: 0854
2025-11-10 23:48:47,004:INFO:self._variable_keys: {'logging_param', 'fold_shuffle_param', 'y', 'y_train', 'idx', 'is_multiclass', 'gpu_param', 'data', 'seed', '_ml_usecase', 'html_param', 'fold_generator', 'exp_name_log', 'y_test', 'gpu_n_jobs_param', 'log_plots_param', '_available_plots', 'fold_groups_param', 'X', 'X_train', 'fix_imbalance', 'pipeline', 'n_jobs_param', 'target_param', 'memory', 'USI', 'exp_id', 'X_test'}
2025-11-10 23:48:47,004:INFO:Checking environment
2025-11-10 23:48:47,004:INFO:python_version: 3.12.3
2025-11-10 23:48:47,004:INFO:python_build: ('main', 'Jun 18 2025 17:59:45')
2025-11-10 23:48:47,004:INFO:machine: x86_64
2025-11-10 23:48:47,005:INFO:platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
2025-11-10 23:48:47,006:INFO:Memory: svmem(total=3835613184, available=3050819584, percent=20.5, used=784793600, free=2649653248, active=114319360, inactive=718196736, buffers=31035392, cached=537874432, shared=3293184, slab=102785024)
2025-11-10 23:48:47,007:INFO:Physical Core: 6
2025-11-10 23:48:47,007:INFO:Logical Core: 12
2025-11-10 23:48:47,007:INFO:Checking libraries
2025-11-10 23:48:47,008:INFO:System:
2025-11-10 23:48:47,008:INFO:    python: 3.12.3 (main, Jun 18 2025, 17:59:45) [GCC 13.3.0]
2025-11-10 23:48:47,008:INFO:executable: /usr/bin/python3
2025-11-10 23:48:47,008:INFO:   machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39
2025-11-10 23:48:47,008:INFO:PyCaret required dependencies:
2025-11-10 23:48:47,015:INFO:                 pip: 25.3
2025-11-10 23:48:47,015:INFO:          setuptools: 68.1.2
2025-11-10 23:48:47,016:INFO:             pycaret: 3.3.2
2025-11-10 23:48:47,016:INFO:             IPython: 9.7.0
2025-11-10 23:48:47,016:INFO:          ipywidgets: 8.1.8
2025-11-10 23:48:47,016:INFO:                tqdm: 4.67.1
2025-11-10 23:48:47,016:INFO:               numpy: 1.26.4
2025-11-10 23:48:47,016:INFO:              pandas: 2.1.4
2025-11-10 23:48:47,017:INFO:              jinja2: 3.1.2
2025-11-10 23:48:47,017:INFO:               scipy: 1.11.4
2025-11-10 23:48:47,017:INFO:              joblib: 1.3.2
2025-11-10 23:48:47,017:INFO:             sklearn: 1.4.2
2025-11-10 23:48:47,017:INFO:                pyod: 2.0.5
2025-11-10 23:48:47,017:INFO:            imblearn: 0.14.0
2025-11-10 23:48:47,018:INFO:   category_encoders: 2.7.0
2025-11-10 23:48:47,018:INFO:               numba: 0.62.1
2025-11-10 23:48:47,018:INFO:            requests: 2.31.0
2025-11-10 23:48:47,018:INFO:          matplotlib: 3.7.5
2025-11-10 23:48:47,018:INFO:          scikitplot: 0.3.7
2025-11-10 23:48:47,018:INFO:         yellowbrick: 1.5
2025-11-10 23:48:47,019:INFO:              plotly: 6.4.0
2025-11-10 23:48:47,019:INFO:    plotly-resampler: Not installed
2025-11-10 23:48:47,019:INFO:             kaleido: 1.2.0
2025-11-10 23:48:47,019:INFO:           schemdraw: 0.15
2025-11-10 23:48:47,019:INFO:         statsmodels: 0.14.5
2025-11-10 23:48:47,019:INFO:              sktime: 0.26.0
2025-11-10 23:48:47,019:INFO:               tbats: 1.1.3
2025-11-10 23:48:47,020:INFO:            pmdarima: 2.0.4
2025-11-10 23:48:47,020:INFO:              psutil: 7.1.3
2025-11-10 23:48:47,020:INFO:          markupsafe: 2.1.5
2025-11-10 23:48:47,020:INFO:             pickle5: Not installed
2025-11-10 23:48:47,020:INFO:         cloudpickle: 3.1.2
2025-11-10 23:48:47,020:INFO:         deprecation: 2.1.0
2025-11-10 23:48:47,021:INFO:              xxhash: 3.6.0
2025-11-10 23:48:47,021:INFO:           wurlitzer: 3.1.1
2025-11-10 23:48:47,021:INFO:PyCaret optional dependencies:
2025-11-10 23:48:47,155:INFO:                shap: Not installed
2025-11-10 23:48:47,156:INFO:           interpret: Not installed
2025-11-10 23:48:47,156:INFO:                umap: 0.5.9.post2
2025-11-10 23:48:47,156:INFO:     ydata_profiling: Not installed
2025-11-10 23:48:47,156:INFO:  explainerdashboard: Not installed
2025-11-10 23:48:47,156:INFO:             autoviz: Not installed
2025-11-10 23:48:47,157:INFO:           fairlearn: Not installed
2025-11-10 23:48:47,157:INFO:          deepchecks: Not installed
2025-11-10 23:48:47,157:INFO:             xgboost: 3.1.1
2025-11-10 23:48:47,157:INFO:            catboost: Not installed
2025-11-10 23:48:47,158:INFO:              kmodes: Not installed
2025-11-10 23:48:47,158:INFO:             mlxtend: Not installed
2025-11-10 23:48:47,158:INFO:       statsforecast: Not installed
2025-11-10 23:48:47,158:INFO:        tune_sklearn: Not installed
2025-11-10 23:48:47,158:INFO:                 ray: Not installed
2025-11-10 23:48:47,158:INFO:            hyperopt: Not installed
2025-11-10 23:48:47,159:INFO:              optuna: Not installed
2025-11-10 23:48:47,159:INFO:               skopt: Not installed
2025-11-10 23:48:47,159:INFO:              mlflow: Not installed
2025-11-10 23:48:47,159:INFO:              gradio: Not installed
2025-11-10 23:48:47,159:INFO:             fastapi: Not installed
2025-11-10 23:48:47,160:INFO:             uvicorn: Not installed
2025-11-10 23:48:47,160:INFO:              m2cgen: Not installed
2025-11-10 23:48:47,160:INFO:           evidently: Not installed
2025-11-10 23:48:47,160:INFO:               fugue: Not installed
2025-11-10 23:48:47,160:INFO:           streamlit: Not installed
2025-11-10 23:48:47,160:INFO:             prophet: Not installed
2025-11-10 23:48:47,161:INFO:None
2025-11-10 23:48:47,161:INFO:Set up data.
2025-11-10 23:48:47,177:INFO:Set up folding strategy.
2025-11-10 23:48:47,177:INFO:Set up train/test split.
2025-11-10 23:48:47,187:INFO:Set up index.
2025-11-10 23:48:47,187:INFO:Assigning column types.
2025-11-10 23:48:47,198:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-11-10 23:48:47,231:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-11-10 23:48:47,234:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-11-10 23:48:47,260:INFO:Soft dependency imported: xgboost: 3.1.1
2025-11-10 23:48:47,296:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-10 23:48:47,330:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-11-10 23:48:47,331:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-11-10 23:48:47,352:INFO:Soft dependency imported: xgboost: 3.1.1
2025-11-10 23:48:47,354:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-10 23:48:47,355:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-11-10 23:48:47,388:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-11-10 23:48:47,408:INFO:Soft dependency imported: xgboost: 3.1.1
2025-11-10 23:48:47,411:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-10 23:48:47,443:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-11-10 23:48:47,462:INFO:Soft dependency imported: xgboost: 3.1.1
2025-11-10 23:48:47,464:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-10 23:48:47,465:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-11-10 23:48:47,516:INFO:Soft dependency imported: xgboost: 3.1.1
2025-11-10 23:48:47,519:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-10 23:48:47,572:INFO:Soft dependency imported: xgboost: 3.1.1
2025-11-10 23:48:47,574:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-10 23:48:47,578:INFO:Preparing preprocessing pipeline...
2025-11-10 23:48:47,582:INFO:Set up simple imputation.
2025-11-10 23:48:47,582:INFO:Set up column transformation.
2025-11-10 23:48:47,582:INFO:Set up feature normalization.
2025-11-10 23:48:47,897:INFO:Finished creating preprocessing pipeline.
2025-11-10 23:48:47,904:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_s...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-11-10 23:48:47,904:INFO:Creating final display dataframe.
2025-11-10 23:48:48,084:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target          label_id
2                   Target type        Multiclass
3           Original data shape        (1641, 79)
4        Transformed data shape        (1641, 79)
5   Transformed train set shape        (1312, 79)
6    Transformed test set shape         (329, 79)
7              Numeric features                78
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Transformation              True
13        Transformation method       yeo-johnson
14                    Normalize              True
15             Normalize method            zscore
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              0854
2025-11-10 23:48:48,145:INFO:Soft dependency imported: xgboost: 3.1.1
2025-11-10 23:48:48,147:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-10 23:48:48,211:INFO:Soft dependency imported: xgboost: 3.1.1
2025-11-10 23:48:48,214:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-10 23:48:48,215:INFO:setup() successfully completed in 1.22s...............
2025-11-10 23:48:48,216:INFO:Initializing compare_models()
2025-11-10 23:48:48,216:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-11-10 23:48:48,216:INFO:Checking exceptions
2025-11-10 23:48:48,229:INFO:Preparing display monitor
2025-11-10 23:48:48,236:INFO:Initializing Logistic Regression
2025-11-10 23:48:48,237:INFO:Total runtime is 7.665157318115234e-06 minutes
2025-11-10 23:48:48,237:INFO:SubProcess create_model() called ==================================
2025-11-10 23:48:48,238:INFO:Initializing create_model()
2025-11-10 23:48:48,238:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:48:48,238:INFO:Checking exceptions
2025-11-10 23:48:48,238:INFO:Importing libraries
2025-11-10 23:48:48,239:INFO:Copying training dataset
2025-11-10 23:48:48,253:INFO:Defining folds
2025-11-10 23:48:48,254:INFO:Declaring metric variables
2025-11-10 23:48:48,254:INFO:Importing untrained model
2025-11-10 23:48:48,255:INFO:Logistic Regression Imported successfully
2025-11-10 23:48:48,255:INFO:Starting cross validation
2025-11-10 23:48:48,257:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:48:49,795:WARNING:/usr/lib/python3.12/multiprocessing/queues.py:122: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  return _ForkingPickler.loads(res)

2025-11-10 23:48:49,795:WARNING:/usr/lib/python3.12/multiprocessing/queues.py:122: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  return _ForkingPickler.loads(res)

2025-11-10 23:48:49,796:WARNING:/usr/lib/python3.12/multiprocessing/queues.py:122: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  return _ForkingPickler.loads(res)

2025-11-10 23:48:49,854:WARNING:/usr/lib/python3.12/multiprocessing/queues.py:122: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  return _ForkingPickler.loads(res)

2025-11-10 23:48:49,860:WARNING:/usr/lib/python3.12/multiprocessing/queues.py:122: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  return _ForkingPickler.loads(res)

2025-11-10 23:48:49,916:WARNING:/usr/lib/python3.12/multiprocessing/queues.py:122: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  return _ForkingPickler.loads(res)

2025-11-10 23:48:49,953:WARNING:/usr/lib/python3.12/multiprocessing/queues.py:122: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  return _ForkingPickler.loads(res)

2025-11-10 23:48:49,953:WARNING:/usr/lib/python3.12/multiprocessing/queues.py:122: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  return _ForkingPickler.loads(res)

2025-11-10 23:48:49,976:WARNING:/usr/lib/python3.12/multiprocessing/queues.py:122: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  return _ForkingPickler.loads(res)

2025-11-10 23:48:49,978:WARNING:/usr/lib/python3.12/multiprocessing/queues.py:122: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  return _ForkingPickler.loads(res)

2025-11-10 23:48:53,908:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:54,008:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:54,106:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:54,136:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:54,169:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:54,173:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:54,212:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:54,224:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:54,251:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:54,324:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:54,345:INFO:Calculating mean and std
2025-11-10 23:48:54,346:INFO:Creating metrics dataframe
2025-11-10 23:48:54,350:INFO:Uploading results into container
2025-11-10 23:48:54,351:INFO:Uploading model into container now
2025-11-10 23:48:54,352:INFO:_master_model_container: 1
2025-11-10 23:48:54,352:INFO:_display_container: 2
2025-11-10 23:48:54,353:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-11-10 23:48:54,354:INFO:create_model() successfully completed......................................
2025-11-10 23:48:54,477:INFO:SubProcess create_model() end ==================================
2025-11-10 23:48:54,478:INFO:Creating metrics dataframe
2025-11-10 23:48:54,480:INFO:Initializing K Neighbors Classifier
2025-11-10 23:48:54,481:INFO:Total runtime is 0.10407671133677165 minutes
2025-11-10 23:48:54,481:INFO:SubProcess create_model() called ==================================
2025-11-10 23:48:54,481:INFO:Initializing create_model()
2025-11-10 23:48:54,481:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:48:54,482:INFO:Checking exceptions
2025-11-10 23:48:54,482:INFO:Importing libraries
2025-11-10 23:48:54,482:INFO:Copying training dataset
2025-11-10 23:48:54,492:INFO:Defining folds
2025-11-10 23:48:54,492:INFO:Declaring metric variables
2025-11-10 23:48:54,493:INFO:Importing untrained model
2025-11-10 23:48:54,493:INFO:K Neighbors Classifier Imported successfully
2025-11-10 23:48:54,493:INFO:Starting cross validation
2025-11-10 23:48:54,494:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:48:55,226:WARNING:/usr/lib/python3.12/multiprocessing/queues.py:122: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  return _ForkingPickler.loads(res)

2025-11-10 23:48:55,235:WARNING:/usr/lib/python3.12/multiprocessing/queues.py:122: RuntimeWarning: PyCaret officially supports Python 3.9-3.11. Continuing on an unsupported version may lead to issues.
  return _ForkingPickler.loads(res)

2025-11-10 23:48:56,690:INFO:Calculating mean and std
2025-11-10 23:48:56,691:INFO:Creating metrics dataframe
2025-11-10 23:48:56,694:INFO:Uploading results into container
2025-11-10 23:48:56,695:INFO:Uploading model into container now
2025-11-10 23:48:56,695:INFO:_master_model_container: 2
2025-11-10 23:48:56,696:INFO:_display_container: 2
2025-11-10 23:48:56,696:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-11-10 23:48:56,697:INFO:create_model() successfully completed......................................
2025-11-10 23:48:56,831:INFO:SubProcess create_model() end ==================================
2025-11-10 23:48:56,831:INFO:Creating metrics dataframe
2025-11-10 23:48:56,834:INFO:Initializing Naive Bayes
2025-11-10 23:48:56,834:INFO:Total runtime is 0.14329784313837687 minutes
2025-11-10 23:48:56,834:INFO:SubProcess create_model() called ==================================
2025-11-10 23:48:56,835:INFO:Initializing create_model()
2025-11-10 23:48:56,835:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:48:56,835:INFO:Checking exceptions
2025-11-10 23:48:56,836:INFO:Importing libraries
2025-11-10 23:48:56,836:INFO:Copying training dataset
2025-11-10 23:48:56,848:INFO:Defining folds
2025-11-10 23:48:56,849:INFO:Declaring metric variables
2025-11-10 23:48:56,849:INFO:Importing untrained model
2025-11-10 23:48:56,849:INFO:Naive Bayes Imported successfully
2025-11-10 23:48:56,850:INFO:Starting cross validation
2025-11-10 23:48:56,851:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:48:57,626:INFO:Calculating mean and std
2025-11-10 23:48:57,627:INFO:Creating metrics dataframe
2025-11-10 23:48:57,630:INFO:Uploading results into container
2025-11-10 23:48:57,631:INFO:Uploading model into container now
2025-11-10 23:48:57,631:INFO:_master_model_container: 3
2025-11-10 23:48:57,632:INFO:_display_container: 2
2025-11-10 23:48:57,632:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-11-10 23:48:57,632:INFO:create_model() successfully completed......................................
2025-11-10 23:48:57,739:INFO:SubProcess create_model() end ==================================
2025-11-10 23:48:57,740:INFO:Creating metrics dataframe
2025-11-10 23:48:57,742:INFO:Initializing Decision Tree Classifier
2025-11-10 23:48:57,742:INFO:Total runtime is 0.15843943357467652 minutes
2025-11-10 23:48:57,743:INFO:SubProcess create_model() called ==================================
2025-11-10 23:48:57,743:INFO:Initializing create_model()
2025-11-10 23:48:57,743:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:48:57,743:INFO:Checking exceptions
2025-11-10 23:48:57,743:INFO:Importing libraries
2025-11-10 23:48:57,744:INFO:Copying training dataset
2025-11-10 23:48:57,755:INFO:Defining folds
2025-11-10 23:48:57,756:INFO:Declaring metric variables
2025-11-10 23:48:57,756:INFO:Importing untrained model
2025-11-10 23:48:57,757:INFO:Decision Tree Classifier Imported successfully
2025-11-10 23:48:57,757:INFO:Starting cross validation
2025-11-10 23:48:57,758:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:48:58,555:INFO:Calculating mean and std
2025-11-10 23:48:58,556:INFO:Creating metrics dataframe
2025-11-10 23:48:58,558:INFO:Uploading results into container
2025-11-10 23:48:58,559:INFO:Uploading model into container now
2025-11-10 23:48:58,559:INFO:_master_model_container: 4
2025-11-10 23:48:58,559:INFO:_display_container: 2
2025-11-10 23:48:58,560:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2025-11-10 23:48:58,560:INFO:create_model() successfully completed......................................
2025-11-10 23:48:58,673:INFO:SubProcess create_model() end ==================================
2025-11-10 23:48:58,674:INFO:Creating metrics dataframe
2025-11-10 23:48:58,676:INFO:Initializing SVM - Linear Kernel
2025-11-10 23:48:58,676:INFO:Total runtime is 0.17399874925613404 minutes
2025-11-10 23:48:58,676:INFO:SubProcess create_model() called ==================================
2025-11-10 23:48:58,677:INFO:Initializing create_model()
2025-11-10 23:48:58,677:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:48:58,677:INFO:Checking exceptions
2025-11-10 23:48:58,677:INFO:Importing libraries
2025-11-10 23:48:58,677:INFO:Copying training dataset
2025-11-10 23:48:58,687:INFO:Defining folds
2025-11-10 23:48:58,688:INFO:Declaring metric variables
2025-11-10 23:48:58,688:INFO:Importing untrained model
2025-11-10 23:48:58,689:INFO:SVM - Linear Kernel Imported successfully
2025-11-10 23:48:58,689:INFO:Starting cross validation
2025-11-10 23:48:58,690:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:48:59,440:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:59,472:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:59,480:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:59,489:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:59,512:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:59,534:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:59,539:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:59,542:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:59,560:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:59,575:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:48:59,591:INFO:Calculating mean and std
2025-11-10 23:48:59,592:INFO:Creating metrics dataframe
2025-11-10 23:48:59,594:INFO:Uploading results into container
2025-11-10 23:48:59,595:INFO:Uploading model into container now
2025-11-10 23:48:59,595:INFO:_master_model_container: 5
2025-11-10 23:48:59,596:INFO:_display_container: 2
2025-11-10 23:48:59,596:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-11-10 23:48:59,597:INFO:create_model() successfully completed......................................
2025-11-10 23:48:59,704:INFO:SubProcess create_model() end ==================================
2025-11-10 23:48:59,704:INFO:Creating metrics dataframe
2025-11-10 23:48:59,707:INFO:Initializing Ridge Classifier
2025-11-10 23:48:59,708:INFO:Total runtime is 0.191195813814799 minutes
2025-11-10 23:48:59,708:INFO:SubProcess create_model() called ==================================
2025-11-10 23:48:59,709:INFO:Initializing create_model()
2025-11-10 23:48:59,709:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:48:59,709:INFO:Checking exceptions
2025-11-10 23:48:59,709:INFO:Importing libraries
2025-11-10 23:48:59,710:INFO:Copying training dataset
2025-11-10 23:48:59,723:INFO:Defining folds
2025-11-10 23:48:59,723:INFO:Declaring metric variables
2025-11-10 23:48:59,724:INFO:Importing untrained model
2025-11-10 23:48:59,725:INFO:Ridge Classifier Imported successfully
2025-11-10 23:48:59,725:INFO:Starting cross validation
2025-11-10 23:48:59,726:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:49:00,378:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:00,378:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:00,394:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:00,396:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:00,397:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:00,401:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:00,408:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:00,424:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:00,426:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:00,438:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:00,452:INFO:Calculating mean and std
2025-11-10 23:49:00,453:INFO:Creating metrics dataframe
2025-11-10 23:49:00,456:INFO:Uploading results into container
2025-11-10 23:49:00,458:INFO:Uploading model into container now
2025-11-10 23:49:00,458:INFO:_master_model_container: 6
2025-11-10 23:49:00,459:INFO:_display_container: 2
2025-11-10 23:49:00,459:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-11-10 23:49:00,460:INFO:create_model() successfully completed......................................
2025-11-10 23:49:00,573:INFO:SubProcess create_model() end ==================================
2025-11-10 23:49:00,574:INFO:Creating metrics dataframe
2025-11-10 23:49:00,577:INFO:Initializing Random Forest Classifier
2025-11-10 23:49:00,578:INFO:Total runtime is 0.20569836695988974 minutes
2025-11-10 23:49:00,578:INFO:SubProcess create_model() called ==================================
2025-11-10 23:49:00,579:INFO:Initializing create_model()
2025-11-10 23:49:00,579:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:49:00,580:INFO:Checking exceptions
2025-11-10 23:49:00,580:INFO:Importing libraries
2025-11-10 23:49:00,580:INFO:Copying training dataset
2025-11-10 23:49:00,595:INFO:Defining folds
2025-11-10 23:49:00,596:INFO:Declaring metric variables
2025-11-10 23:49:00,596:INFO:Importing untrained model
2025-11-10 23:49:00,597:INFO:Random Forest Classifier Imported successfully
2025-11-10 23:49:00,598:INFO:Starting cross validation
2025-11-10 23:49:00,599:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:49:02,518:INFO:Calculating mean and std
2025-11-10 23:49:02,519:INFO:Creating metrics dataframe
2025-11-10 23:49:02,520:INFO:Uploading results into container
2025-11-10 23:49:02,521:INFO:Uploading model into container now
2025-11-10 23:49:02,522:INFO:_master_model_container: 7
2025-11-10 23:49:02,522:INFO:_display_container: 2
2025-11-10 23:49:02,522:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-11-10 23:49:02,523:INFO:create_model() successfully completed......................................
2025-11-10 23:49:02,617:INFO:SubProcess create_model() end ==================================
2025-11-10 23:49:02,617:INFO:Creating metrics dataframe
2025-11-10 23:49:02,619:INFO:Initializing Quadratic Discriminant Analysis
2025-11-10 23:49:02,619:INFO:Total runtime is 0.23972436189651491 minutes
2025-11-10 23:49:02,620:INFO:SubProcess create_model() called ==================================
2025-11-10 23:49:02,620:INFO:Initializing create_model()
2025-11-10 23:49:02,620:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:49:02,620:INFO:Checking exceptions
2025-11-10 23:49:02,620:INFO:Importing libraries
2025-11-10 23:49:02,621:INFO:Copying training dataset
2025-11-10 23:49:02,629:INFO:Defining folds
2025-11-10 23:49:02,630:INFO:Declaring metric variables
2025-11-10 23:49:02,630:INFO:Importing untrained model
2025-11-10 23:49:02,631:INFO:Quadratic Discriminant Analysis Imported successfully
2025-11-10 23:49:02,631:INFO:Starting cross validation
2025-11-10 23:49:02,632:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:49:03,334:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-10 23:49:03,334:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-10 23:49:03,337:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-10 23:49:03,340:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-10 23:49:03,341:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-10 23:49:03,343:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-10 23:49:03,363:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-10 23:49:03,368:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-10 23:49:03,371:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:03,373:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:03,373:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-10 23:49:03,376:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:03,378:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:03,380:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:03,382:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:03,384:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-10 23:49:03,393:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:03,397:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:03,404:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:03,417:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:03,431:INFO:Calculating mean and std
2025-11-10 23:49:03,431:INFO:Creating metrics dataframe
2025-11-10 23:49:03,433:INFO:Uploading results into container
2025-11-10 23:49:03,434:INFO:Uploading model into container now
2025-11-10 23:49:03,434:INFO:_master_model_container: 8
2025-11-10 23:49:03,434:INFO:_display_container: 2
2025-11-10 23:49:03,435:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-11-10 23:49:03,435:INFO:create_model() successfully completed......................................
2025-11-10 23:49:03,528:INFO:SubProcess create_model() end ==================================
2025-11-10 23:49:03,528:INFO:Creating metrics dataframe
2025-11-10 23:49:03,531:INFO:Initializing Ada Boost Classifier
2025-11-10 23:49:03,531:INFO:Total runtime is 0.2549137274424235 minutes
2025-11-10 23:49:03,531:INFO:SubProcess create_model() called ==================================
2025-11-10 23:49:03,531:INFO:Initializing create_model()
2025-11-10 23:49:03,532:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:49:03,532:INFO:Checking exceptions
2025-11-10 23:49:03,532:INFO:Importing libraries
2025-11-10 23:49:03,532:INFO:Copying training dataset
2025-11-10 23:49:03,541:INFO:Defining folds
2025-11-10 23:49:03,541:INFO:Declaring metric variables
2025-11-10 23:49:03,541:INFO:Importing untrained model
2025-11-10 23:49:03,542:INFO:Ada Boost Classifier Imported successfully
2025-11-10 23:49:03,542:INFO:Starting cross validation
2025-11-10 23:49:03,543:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:49:04,117:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-10 23:49:04,131:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-10 23:49:04,142:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-10 23:49:04,144:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-10 23:49:04,148:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-10 23:49:04,163:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-10 23:49:04,164:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-10 23:49:04,164:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-10 23:49:04,166:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-10 23:49:04,205:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-10 23:49:05,091:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:05,098:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:05,123:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:05,127:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:05,131:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:05,133:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:05,138:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:05,141:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:05,152:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:05,177:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:05,191:INFO:Calculating mean and std
2025-11-10 23:49:05,192:INFO:Creating metrics dataframe
2025-11-10 23:49:05,194:INFO:Uploading results into container
2025-11-10 23:49:05,195:INFO:Uploading model into container now
2025-11-10 23:49:05,195:INFO:_master_model_container: 9
2025-11-10 23:49:05,195:INFO:_display_container: 2
2025-11-10 23:49:05,196:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2025-11-10 23:49:05,196:INFO:create_model() successfully completed......................................
2025-11-10 23:49:05,307:INFO:SubProcess create_model() end ==================================
2025-11-10 23:49:05,307:INFO:Creating metrics dataframe
2025-11-10 23:49:05,310:INFO:Initializing Gradient Boosting Classifier
2025-11-10 23:49:05,310:INFO:Total runtime is 0.2845652103424072 minutes
2025-11-10 23:49:05,310:INFO:SubProcess create_model() called ==================================
2025-11-10 23:49:05,311:INFO:Initializing create_model()
2025-11-10 23:49:05,311:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:49:05,311:INFO:Checking exceptions
2025-11-10 23:49:05,311:INFO:Importing libraries
2025-11-10 23:49:05,311:INFO:Copying training dataset
2025-11-10 23:49:05,321:INFO:Defining folds
2025-11-10 23:49:05,322:INFO:Declaring metric variables
2025-11-10 23:49:05,323:INFO:Importing untrained model
2025-11-10 23:49:05,323:INFO:Gradient Boosting Classifier Imported successfully
2025-11-10 23:49:05,324:INFO:Starting cross validation
2025-11-10 23:49:05,325:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:49:16,573:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:16,589:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:16,592:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:16,594:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:16,604:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:16,607:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:16,609:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:16,628:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:16,629:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:16,633:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:16,647:INFO:Calculating mean and std
2025-11-10 23:49:16,648:INFO:Creating metrics dataframe
2025-11-10 23:49:16,650:INFO:Uploading results into container
2025-11-10 23:49:16,651:INFO:Uploading model into container now
2025-11-10 23:49:16,651:INFO:_master_model_container: 10
2025-11-10 23:49:16,652:INFO:_display_container: 2
2025-11-10 23:49:16,652:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-11-10 23:49:16,652:INFO:create_model() successfully completed......................................
2025-11-10 23:49:16,744:INFO:SubProcess create_model() end ==================================
2025-11-10 23:49:16,745:INFO:Creating metrics dataframe
2025-11-10 23:49:16,747:INFO:Initializing Linear Discriminant Analysis
2025-11-10 23:49:16,747:INFO:Total runtime is 0.47518237829208376 minutes
2025-11-10 23:49:16,747:INFO:SubProcess create_model() called ==================================
2025-11-10 23:49:16,748:INFO:Initializing create_model()
2025-11-10 23:49:16,748:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:49:16,748:INFO:Checking exceptions
2025-11-10 23:49:16,748:INFO:Importing libraries
2025-11-10 23:49:16,748:INFO:Copying training dataset
2025-11-10 23:49:16,757:INFO:Defining folds
2025-11-10 23:49:16,757:INFO:Declaring metric variables
2025-11-10 23:49:16,758:INFO:Importing untrained model
2025-11-10 23:49:16,758:INFO:Linear Discriminant Analysis Imported successfully
2025-11-10 23:49:16,758:INFO:Starting cross validation
2025-11-10 23:49:16,759:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:49:17,379:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:17,379:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:17,379:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:17,379:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:17,380:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:17,383:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:17,400:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:17,404:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:17,405:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:17,411:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-10 23:49:17,431:INFO:Calculating mean and std
2025-11-10 23:49:17,432:INFO:Creating metrics dataframe
2025-11-10 23:49:17,433:INFO:Uploading results into container
2025-11-10 23:49:17,434:INFO:Uploading model into container now
2025-11-10 23:49:17,435:INFO:_master_model_container: 11
2025-11-10 23:49:17,435:INFO:_display_container: 2
2025-11-10 23:49:17,435:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-11-10 23:49:17,436:INFO:create_model() successfully completed......................................
2025-11-10 23:49:17,529:INFO:SubProcess create_model() end ==================================
2025-11-10 23:49:17,530:INFO:Creating metrics dataframe
2025-11-10 23:49:17,532:INFO:Initializing Extra Trees Classifier
2025-11-10 23:49:17,532:INFO:Total runtime is 0.48827253182729086 minutes
2025-11-10 23:49:17,533:INFO:SubProcess create_model() called ==================================
2025-11-10 23:49:17,533:INFO:Initializing create_model()
2025-11-10 23:49:17,533:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:49:17,533:INFO:Checking exceptions
2025-11-10 23:49:17,533:INFO:Importing libraries
2025-11-10 23:49:17,534:INFO:Copying training dataset
2025-11-10 23:49:17,543:INFO:Defining folds
2025-11-10 23:49:17,543:INFO:Declaring metric variables
2025-11-10 23:49:17,543:INFO:Importing untrained model
2025-11-10 23:49:17,544:INFO:Extra Trees Classifier Imported successfully
2025-11-10 23:49:17,544:INFO:Starting cross validation
2025-11-10 23:49:17,545:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:49:18,759:INFO:Calculating mean and std
2025-11-10 23:49:18,760:INFO:Creating metrics dataframe
2025-11-10 23:49:18,762:INFO:Uploading results into container
2025-11-10 23:49:18,763:INFO:Uploading model into container now
2025-11-10 23:49:18,763:INFO:_master_model_container: 12
2025-11-10 23:49:18,764:INFO:_display_container: 2
2025-11-10 23:49:18,764:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-11-10 23:49:18,765:INFO:create_model() successfully completed......................................
2025-11-10 23:49:18,860:INFO:SubProcess create_model() end ==================================
2025-11-10 23:49:18,862:INFO:Creating metrics dataframe
2025-11-10 23:49:18,864:INFO:Initializing Extreme Gradient Boosting
2025-11-10 23:49:18,865:INFO:Total runtime is 0.5104745070139567 minutes
2025-11-10 23:49:18,865:INFO:SubProcess create_model() called ==================================
2025-11-10 23:49:18,865:INFO:Initializing create_model()
2025-11-10 23:49:18,865:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:49:18,866:INFO:Checking exceptions
2025-11-10 23:49:18,866:INFO:Importing libraries
2025-11-10 23:49:18,866:INFO:Copying training dataset
2025-11-10 23:49:18,875:INFO:Defining folds
2025-11-10 23:49:18,875:INFO:Declaring metric variables
2025-11-10 23:49:18,876:INFO:Importing untrained model
2025-11-10 23:49:18,876:INFO:Extreme Gradient Boosting Imported successfully
2025-11-10 23:49:18,877:INFO:Starting cross validation
2025-11-10 23:49:18,877:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:49:23,636:INFO:Calculating mean and std
2025-11-10 23:49:23,637:INFO:Creating metrics dataframe
2025-11-10 23:49:23,639:INFO:Uploading results into container
2025-11-10 23:49:23,640:INFO:Uploading model into container now
2025-11-10 23:49:23,640:INFO:_master_model_container: 13
2025-11-10 23:49:23,640:INFO:_display_container: 2
2025-11-10 23:49:23,641:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              feature_weights=None, gamma=None, grow_policy=None,
              importance_type=None, interaction_constraints=None,
              learning_rate=None, max_bin=None, max_cat_threshold=None,
              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,
              max_leaves=None, min_child_weight=None, missing=nan,
              monotone_constraints=None, multi_strategy=None, n_estimators=None,
              n_jobs=-1, num_parallel_tree=None, ...)
2025-11-10 23:49:23,641:INFO:create_model() successfully completed......................................
2025-11-10 23:49:23,741:INFO:SubProcess create_model() end ==================================
2025-11-10 23:49:23,742:INFO:Creating metrics dataframe
2025-11-10 23:49:23,744:INFO:Initializing Light Gradient Boosting Machine
2025-11-10 23:49:23,744:INFO:Total runtime is 0.5918032884597778 minutes
2025-11-10 23:49:23,745:INFO:SubProcess create_model() called ==================================
2025-11-10 23:49:23,745:INFO:Initializing create_model()
2025-11-10 23:49:23,745:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:49:23,745:INFO:Checking exceptions
2025-11-10 23:49:23,745:INFO:Importing libraries
2025-11-10 23:49:23,746:INFO:Copying training dataset
2025-11-10 23:49:23,754:INFO:Defining folds
2025-11-10 23:49:23,754:INFO:Declaring metric variables
2025-11-10 23:49:23,754:INFO:Importing untrained model
2025-11-10 23:49:23,755:INFO:Light Gradient Boosting Machine Imported successfully
2025-11-10 23:49:23,755:INFO:Starting cross validation
2025-11-10 23:49:23,756:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:49:28,586:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-10 23:49:28,642:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-10 23:49:28,686:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-10 23:49:28,717:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-10 23:49:28,736:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-10 23:49:28,757:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-10 23:49:28,768:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-10 23:49:28,769:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-10 23:49:28,799:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-10 23:49:28,856:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 345, in _score
    y_pred = method_caller(
             ^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
                ^^^^^^^^^^^^^^^^^^^^^
  File "/home/maraw/.local/lib/python3.12/site-packages/sklearn/utils/_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-10 23:49:28,868:INFO:Calculating mean and std
2025-11-10 23:49:28,869:INFO:Creating metrics dataframe
2025-11-10 23:49:28,870:INFO:Uploading results into container
2025-11-10 23:49:28,871:INFO:Uploading model into container now
2025-11-10 23:49:28,872:INFO:_master_model_container: 14
2025-11-10 23:49:28,872:INFO:_display_container: 2
2025-11-10 23:49:28,872:INFO:<lightgbm.LGBMClassifier object at 0x7f3870d5bb00>
2025-11-10 23:49:28,872:INFO:create_model() successfully completed......................................
2025-11-10 23:49:28,984:INFO:SubProcess create_model() end ==================================
2025-11-10 23:49:28,985:INFO:Creating metrics dataframe
2025-11-10 23:49:28,987:INFO:Initializing Dummy Classifier
2025-11-10 23:49:28,987:INFO:Total runtime is 0.6791905283927917 minutes
2025-11-10 23:49:28,988:INFO:SubProcess create_model() called ==================================
2025-11-10 23:49:28,988:INFO:Initializing create_model()
2025-11-10 23:49:28,989:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f3870c70bf0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:49:28,989:INFO:Checking exceptions
2025-11-10 23:49:28,989:INFO:Importing libraries
2025-11-10 23:49:28,989:INFO:Copying training dataset
2025-11-10 23:49:29,000:INFO:Defining folds
2025-11-10 23:49:29,001:INFO:Declaring metric variables
2025-11-10 23:49:29,001:INFO:Importing untrained model
2025-11-10 23:49:29,002:INFO:Dummy Classifier Imported successfully
2025-11-10 23:49:29,002:INFO:Starting cross validation
2025-11-10 23:49:29,003:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-10 23:49:29,665:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-10 23:49:29,667:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-10 23:49:29,668:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-10 23:49:29,674:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-10 23:49:29,679:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-10 23:49:29,679:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-10 23:49:29,689:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-10 23:49:29,692:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-10 23:49:29,697:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-10 23:49:29,698:WARNING:/home/maraw/.local/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-10 23:49:29,713:INFO:Calculating mean and std
2025-11-10 23:49:29,714:INFO:Creating metrics dataframe
2025-11-10 23:49:29,716:INFO:Uploading results into container
2025-11-10 23:49:29,717:INFO:Uploading model into container now
2025-11-10 23:49:29,717:INFO:_master_model_container: 15
2025-11-10 23:49:29,718:INFO:_display_container: 2
2025-11-10 23:49:29,718:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2025-11-10 23:49:29,718:INFO:create_model() successfully completed......................................
2025-11-10 23:49:29,826:INFO:SubProcess create_model() end ==================================
2025-11-10 23:49:29,827:INFO:Creating metrics dataframe
2025-11-10 23:49:29,832:WARNING:/home/maraw/.local/lib/python3.12/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-11-10 23:49:29,833:INFO:Initializing create_model()
2025-11-10 23:49:29,834:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:49:29,834:INFO:Checking exceptions
2025-11-10 23:49:29,834:INFO:Importing libraries
2025-11-10 23:49:29,835:INFO:Copying training dataset
2025-11-10 23:49:29,844:INFO:Defining folds
2025-11-10 23:49:29,845:INFO:Declaring metric variables
2025-11-10 23:49:29,845:INFO:Importing untrained model
2025-11-10 23:49:29,845:INFO:Declaring custom model
2025-11-10 23:49:29,846:INFO:Gradient Boosting Classifier Imported successfully
2025-11-10 23:49:29,847:INFO:Cross validation set to False
2025-11-10 23:49:29,848:INFO:Fitting Model
2025-11-10 23:49:36,265:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-11-10 23:49:36,265:INFO:create_model() successfully completed......................................
2025-11-10 23:49:36,369:INFO:_master_model_container: 15
2025-11-10 23:49:36,369:INFO:_display_container: 2
2025-11-10 23:49:36,370:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-11-10 23:49:36,370:INFO:compare_models() successfully completed......................................
2025-11-10 23:49:36,370:INFO:Initializing finalize_model()
2025-11-10 23:49:36,371:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-11-10 23:49:36,371:INFO:Finalizing GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-11-10 23:49:36,377:INFO:Initializing create_model()
2025-11-10 23:49:36,377:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-11-10 23:49:36,378:INFO:Checking exceptions
2025-11-10 23:49:36,379:INFO:Importing libraries
2025-11-10 23:49:36,379:INFO:Copying training dataset
2025-11-10 23:49:36,380:INFO:Defining folds
2025-11-10 23:49:36,380:INFO:Declaring metric variables
2025-11-10 23:49:36,381:INFO:Importing untrained model
2025-11-10 23:49:36,381:INFO:Declaring custom model
2025-11-10 23:49:36,382:INFO:Gradient Boosting Classifier Imported successfully
2025-11-10 23:49:36,382:INFO:Cross validation set to False
2025-11-10 23:49:36,383:INFO:Fitting Model
2025-11-10 23:49:44,630:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2025-11-10 23:49:44,630:INFO:create_model() successfully completed......................................
2025-11-10 23:49:44,725:INFO:_master_model_container: 15
2025-11-10 23:49:44,726:INFO:_display_container: 2
2025-11-10 23:49:44,731:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2025-11-10 23:49:44,731:INFO:finalize_model() successfully completed......................................
2025-11-10 23:49:44,823:INFO:Initializing predict_model()
2025-11-10 23:49:44,823:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7f38710c7e30>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f3870dcfec0>)
2025-11-10 23:49:44,823:INFO:Checking exceptions
2025-11-10 23:49:44,823:INFO:Preloading libraries
2025-11-10 23:49:45,065:INFO:Initializing save_model()
2025-11-10 23:49:45,066:INFO:save_model(model=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False), model_name=reports/phase3/pycaret/best_model, prep_pipe_=Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_s...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-11-10 23:49:45,066:INFO:Adding model into prep_pipe
2025-11-10 23:49:45,066:WARNING:Only Model saved as it was a pipeline.
2025-11-10 23:49:45,085:INFO:reports/phase3/pycaret/best_model.pkl saved in current working directory
2025-11-10 23:49:45,090:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2025-11-10 23:49:45,091:INFO:save_model() successfully completed......................................
2025-11-11 00:50:37,267:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-11 00:50:37,267:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-11 00:50:37,267:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-11 00:50:37,267:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-11 00:50:39,939:INFO:PyCaret ClassificationExperiment
2025-11-11 00:50:39,939:INFO:Logging name: clf-default-name
2025-11-11 00:50:39,939:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-11-11 00:50:39,939:INFO:version 3.3.2
2025-11-11 00:50:39,939:INFO:Initializing setup()
2025-11-11 00:50:39,939:INFO:self.USI: 015c
2025-11-11 00:50:39,939:INFO:self._variable_keys: {'_available_plots', 'exp_name_log', 'memory', 'html_param', 'USI', 'exp_id', 'X_train', 'X_test', 'n_jobs_param', 'is_multiclass', '_ml_usecase', 'idx', 'y_test', 'y_train', 'fold_shuffle_param', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'fold_generator', 'fix_imbalance', 'X', 'log_plots_param', 'fold_groups_param', 'pipeline', 'y', 'target_param', 'logging_param', 'data'}
2025-11-11 00:50:39,939:INFO:Checking environment
2025-11-11 00:50:39,939:INFO:python_version: 3.9.21
2025-11-11 00:50:39,939:INFO:python_build: ('main', 'Dec 11 2024 16:35:24')
2025-11-11 00:50:39,939:INFO:machine: AMD64
2025-11-11 00:50:39,956:INFO:platform: Windows-10-10.0.26200-SP0
2025-11-11 00:50:39,956:INFO:Memory: svmem(total=7897935872, available=1144901632, percent=85.5, used=6753034240, free=1144901632)
2025-11-11 00:50:39,956:INFO:Physical Core: 6
2025-11-11 00:50:39,956:INFO:Logical Core: 12
2025-11-11 00:50:39,956:INFO:Checking libraries
2025-11-11 00:50:39,956:INFO:System:
2025-11-11 00:50:39,956:INFO:    python: 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]
2025-11-11 00:50:39,957:INFO:executable: C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\Scripts\python.exe
2025-11-11 00:50:39,957:INFO:   machine: Windows-10-10.0.26200-SP0
2025-11-11 00:50:39,957:INFO:PyCaret required dependencies:
2025-11-11 00:50:40,004:INFO:                 pip: 25.3
2025-11-11 00:50:40,005:INFO:          setuptools: 58.1.0
2025-11-11 00:50:40,005:INFO:             pycaret: 3.3.2
2025-11-11 00:50:40,005:INFO:             IPython: 8.18.1
2025-11-11 00:50:40,005:INFO:          ipywidgets: 8.1.8
2025-11-11 00:50:40,005:INFO:                tqdm: 4.67.1
2025-11-11 00:50:40,005:INFO:               numpy: 1.26.4
2025-11-11 00:50:40,005:INFO:              pandas: 2.1.4
2025-11-11 00:50:40,005:INFO:              jinja2: 3.1.6
2025-11-11 00:50:40,005:INFO:               scipy: 1.11.4
2025-11-11 00:50:40,005:INFO:              joblib: 1.3.2
2025-11-11 00:50:40,005:INFO:             sklearn: 1.4.2
2025-11-11 00:50:40,005:INFO:                pyod: 2.0.5
2025-11-11 00:50:40,005:INFO:            imblearn: 0.12.4
2025-11-11 00:50:40,005:INFO:   category_encoders: 2.6.4
2025-11-11 00:50:40,005:INFO:            lightgbm: Installed but version unavailable
2025-11-11 00:50:40,005:INFO:               numba: 0.60.0
2025-11-11 00:50:40,005:INFO:            requests: 2.32.5
2025-11-11 00:50:40,006:INFO:          matplotlib: 3.7.5
2025-11-11 00:50:40,006:INFO:          scikitplot: 0.3.7
2025-11-11 00:50:40,006:INFO:         yellowbrick: 1.5
2025-11-11 00:50:40,006:INFO:              plotly: 6.4.0
2025-11-11 00:50:40,006:INFO:    plotly-resampler: Not installed
2025-11-11 00:50:40,006:INFO:             kaleido: 1.2.0
2025-11-11 00:50:40,006:INFO:           schemdraw: 0.15
2025-11-11 00:50:40,006:INFO:         statsmodels: 0.14.5
2025-11-11 00:50:40,006:INFO:              sktime: 0.26.0
2025-11-11 00:50:40,006:INFO:               tbats: 1.1.3
2025-11-11 00:50:40,006:INFO:            pmdarima: 2.0.4
2025-11-11 00:50:40,006:INFO:              psutil: 7.1.3
2025-11-11 00:50:40,006:INFO:          markupsafe: 3.0.3
2025-11-11 00:50:40,006:INFO:             pickle5: Not installed
2025-11-11 00:50:40,006:INFO:         cloudpickle: 3.1.2
2025-11-11 00:50:40,006:INFO:         deprecation: 2.1.0
2025-11-11 00:50:40,006:INFO:              xxhash: 3.6.0
2025-11-11 00:50:40,006:INFO:           wurlitzer: Not installed
2025-11-11 00:50:40,006:INFO:PyCaret optional dependencies:
2025-11-11 00:50:40,082:INFO:                shap: Not installed
2025-11-11 00:50:40,082:INFO:           interpret: Not installed
2025-11-11 00:50:40,082:INFO:                umap: 0.5.4
2025-11-11 00:50:40,082:INFO:     ydata_profiling: Not installed
2025-11-11 00:50:40,082:INFO:  explainerdashboard: Not installed
2025-11-11 00:50:40,082:INFO:             autoviz: Not installed
2025-11-11 00:50:40,082:INFO:           fairlearn: Not installed
2025-11-11 00:50:40,082:INFO:          deepchecks: Not installed
2025-11-11 00:50:40,082:INFO:             xgboost: 2.0.3
2025-11-11 00:50:40,082:INFO:            catboost: Not installed
2025-11-11 00:50:40,082:INFO:              kmodes: Not installed
2025-11-11 00:50:40,082:INFO:             mlxtend: Not installed
2025-11-11 00:50:40,082:INFO:       statsforecast: Not installed
2025-11-11 00:50:40,082:INFO:        tune_sklearn: Not installed
2025-11-11 00:50:40,082:INFO:                 ray: Not installed
2025-11-11 00:50:40,082:INFO:            hyperopt: Not installed
2025-11-11 00:50:40,082:INFO:              optuna: Not installed
2025-11-11 00:50:40,082:INFO:               skopt: Not installed
2025-11-11 00:50:40,082:INFO:              mlflow: Not installed
2025-11-11 00:50:40,083:INFO:              gradio: Not installed
2025-11-11 00:50:40,083:INFO:             fastapi: Not installed
2025-11-11 00:50:40,083:INFO:             uvicorn: Not installed
2025-11-11 00:50:40,083:INFO:              m2cgen: Not installed
2025-11-11 00:50:40,083:INFO:           evidently: Not installed
2025-11-11 00:50:40,083:INFO:               fugue: Not installed
2025-11-11 00:50:40,083:INFO:           streamlit: Not installed
2025-11-11 00:50:40,083:INFO:             prophet: Not installed
2025-11-11 00:50:40,083:INFO:None
2025-11-11 00:50:40,083:INFO:Set up data.
2025-11-11 00:50:40,109:INFO:Set up folding strategy.
2025-11-11 00:50:40,109:INFO:Set up train/test split.
2025-11-11 00:50:40,131:INFO:Set up index.
2025-11-11 00:50:40,135:INFO:Assigning column types.
2025-11-11 00:50:40,160:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-11-11 00:50:40,235:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-11-11 00:50:40,241:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-11-11 00:50:40,314:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-11 00:50:40,317:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-11 00:50:40,379:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-11-11 00:50:40,379:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-11-11 00:50:40,426:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-11 00:50:40,430:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-11 00:50:40,430:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-11-11 00:50:40,510:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-11-11 00:50:40,556:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-11 00:50:40,562:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-11 00:50:40,646:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-11-11 00:50:40,694:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-11 00:50:40,697:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-11 00:50:40,697:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-11-11 00:50:40,799:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-11 00:50:40,805:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-11 00:50:40,897:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-11 00:50:40,897:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-11 00:50:40,909:INFO:Preparing preprocessing pipeline...
2025-11-11 00:50:40,913:INFO:Set up simple imputation.
2025-11-11 00:50:40,915:INFO:Set up column transformation.
2025-11-11 00:50:40,915:INFO:Set up feature normalization.
2025-11-11 00:50:41,532:INFO:Finished creating preprocessing pipeline.
2025-11-11 00:50:41,541:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\maraw\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_fa...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-11-11 00:50:41,541:INFO:Creating final display dataframe.
2025-11-11 00:50:41,991:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target          label_id
2                   Target type        Multiclass
3           Original data shape        (1641, 79)
4        Transformed data shape        (1641, 79)
5   Transformed train set shape        (1312, 79)
6    Transformed test set shape         (329, 79)
7              Numeric features                78
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Transformation              True
13        Transformation method       yeo-johnson
14                    Normalize              True
15             Normalize method            zscore
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              015c
2025-11-11 00:50:42,126:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-11 00:50:42,132:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-11 00:50:42,247:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-11 00:50:42,249:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-11 00:50:42,251:INFO:setup() successfully completed in 2.32s...............
2025-11-11 00:50:42,251:INFO:Initializing compare_models()
2025-11-11 00:50:42,251:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-11-11 00:50:42,251:INFO:Checking exceptions
2025-11-11 00:50:42,272:INFO:Preparing display monitor
2025-11-11 00:50:42,280:INFO:Initializing Logistic Regression
2025-11-11 00:50:42,280:INFO:Total runtime is 0.0 minutes
2025-11-11 00:50:42,280:INFO:SubProcess create_model() called ==================================
2025-11-11 00:50:42,280:INFO:Initializing create_model()
2025-11-11 00:50:42,280:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:50:42,280:INFO:Checking exceptions
2025-11-11 00:50:42,280:INFO:Importing libraries
2025-11-11 00:50:42,280:INFO:Copying training dataset
2025-11-11 00:50:42,302:INFO:Defining folds
2025-11-11 00:50:42,309:INFO:Declaring metric variables
2025-11-11 00:50:42,309:INFO:Importing untrained model
2025-11-11 00:50:42,309:INFO:Logistic Regression Imported successfully
2025-11-11 00:50:42,311:INFO:Starting cross validation
2025-11-11 00:50:42,313:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:50:48,952:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:49,096:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:49,131:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:49,136:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:49,181:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:49,192:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:49,223:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:49,276:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:49,279:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:49,298:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:49,319:INFO:Calculating mean and std
2025-11-11 00:50:49,321:INFO:Creating metrics dataframe
2025-11-11 00:50:49,323:INFO:Uploading results into container
2025-11-11 00:50:49,323:INFO:Uploading model into container now
2025-11-11 00:50:49,325:INFO:_master_model_container: 1
2025-11-11 00:50:49,325:INFO:_display_container: 2
2025-11-11 00:50:49,326:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-11-11 00:50:49,326:INFO:create_model() successfully completed......................................
2025-11-11 00:50:49,417:INFO:SubProcess create_model() end ==================================
2025-11-11 00:50:49,417:INFO:Creating metrics dataframe
2025-11-11 00:50:49,426:INFO:Initializing K Neighbors Classifier
2025-11-11 00:50:49,426:INFO:Total runtime is 0.11908798615137736 minutes
2025-11-11 00:50:49,426:INFO:SubProcess create_model() called ==================================
2025-11-11 00:50:49,426:INFO:Initializing create_model()
2025-11-11 00:50:49,426:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:50:49,426:INFO:Checking exceptions
2025-11-11 00:50:49,426:INFO:Importing libraries
2025-11-11 00:50:49,426:INFO:Copying training dataset
2025-11-11 00:50:49,436:INFO:Defining folds
2025-11-11 00:50:49,436:INFO:Declaring metric variables
2025-11-11 00:50:49,436:INFO:Importing untrained model
2025-11-11 00:50:49,436:INFO:K Neighbors Classifier Imported successfully
2025-11-11 00:50:49,436:INFO:Starting cross validation
2025-11-11 00:50:49,436:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:50:52,337:INFO:Calculating mean and std
2025-11-11 00:50:52,338:INFO:Creating metrics dataframe
2025-11-11 00:50:52,342:INFO:Uploading results into container
2025-11-11 00:50:52,342:INFO:Uploading model into container now
2025-11-11 00:50:52,342:INFO:_master_model_container: 2
2025-11-11 00:50:52,342:INFO:_display_container: 2
2025-11-11 00:50:52,344:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-11-11 00:50:52,344:INFO:create_model() successfully completed......................................
2025-11-11 00:50:52,438:INFO:SubProcess create_model() end ==================================
2025-11-11 00:50:52,438:INFO:Creating metrics dataframe
2025-11-11 00:50:52,442:INFO:Initializing Naive Bayes
2025-11-11 00:50:52,442:INFO:Total runtime is 0.16935856739679972 minutes
2025-11-11 00:50:52,442:INFO:SubProcess create_model() called ==================================
2025-11-11 00:50:52,442:INFO:Initializing create_model()
2025-11-11 00:50:52,442:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:50:52,442:INFO:Checking exceptions
2025-11-11 00:50:52,442:INFO:Importing libraries
2025-11-11 00:50:52,442:INFO:Copying training dataset
2025-11-11 00:50:52,458:INFO:Defining folds
2025-11-11 00:50:52,459:INFO:Declaring metric variables
2025-11-11 00:50:52,459:INFO:Importing untrained model
2025-11-11 00:50:52,459:INFO:Naive Bayes Imported successfully
2025-11-11 00:50:52,459:INFO:Starting cross validation
2025-11-11 00:50:52,460:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:50:53,031:INFO:Calculating mean and std
2025-11-11 00:50:53,031:INFO:Creating metrics dataframe
2025-11-11 00:50:53,035:INFO:Uploading results into container
2025-11-11 00:50:53,035:INFO:Uploading model into container now
2025-11-11 00:50:53,036:INFO:_master_model_container: 3
2025-11-11 00:50:53,036:INFO:_display_container: 2
2025-11-11 00:50:53,036:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-11-11 00:50:53,036:INFO:create_model() successfully completed......................................
2025-11-11 00:50:53,112:INFO:SubProcess create_model() end ==================================
2025-11-11 00:50:53,112:INFO:Creating metrics dataframe
2025-11-11 00:50:53,116:INFO:Initializing Decision Tree Classifier
2025-11-11 00:50:53,116:INFO:Total runtime is 0.1805896242459615 minutes
2025-11-11 00:50:53,116:INFO:SubProcess create_model() called ==================================
2025-11-11 00:50:53,116:INFO:Initializing create_model()
2025-11-11 00:50:53,116:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:50:53,116:INFO:Checking exceptions
2025-11-11 00:50:53,116:INFO:Importing libraries
2025-11-11 00:50:53,116:INFO:Copying training dataset
2025-11-11 00:50:53,127:INFO:Defining folds
2025-11-11 00:50:53,127:INFO:Declaring metric variables
2025-11-11 00:50:53,127:INFO:Importing untrained model
2025-11-11 00:50:53,127:INFO:Decision Tree Classifier Imported successfully
2025-11-11 00:50:53,127:INFO:Starting cross validation
2025-11-11 00:50:53,127:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:50:53,872:INFO:Calculating mean and std
2025-11-11 00:50:53,872:INFO:Creating metrics dataframe
2025-11-11 00:50:53,876:INFO:Uploading results into container
2025-11-11 00:50:53,876:INFO:Uploading model into container now
2025-11-11 00:50:53,876:INFO:_master_model_container: 4
2025-11-11 00:50:53,876:INFO:_display_container: 2
2025-11-11 00:50:53,876:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2025-11-11 00:50:53,876:INFO:create_model() successfully completed......................................
2025-11-11 00:50:53,957:INFO:SubProcess create_model() end ==================================
2025-11-11 00:50:53,957:INFO:Creating metrics dataframe
2025-11-11 00:50:53,957:INFO:Initializing SVM - Linear Kernel
2025-11-11 00:50:53,957:INFO:Total runtime is 0.19461537996927897 minutes
2025-11-11 00:50:53,957:INFO:SubProcess create_model() called ==================================
2025-11-11 00:50:53,957:INFO:Initializing create_model()
2025-11-11 00:50:53,957:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:50:53,957:INFO:Checking exceptions
2025-11-11 00:50:53,957:INFO:Importing libraries
2025-11-11 00:50:53,957:INFO:Copying training dataset
2025-11-11 00:50:53,972:INFO:Defining folds
2025-11-11 00:50:53,972:INFO:Declaring metric variables
2025-11-11 00:50:53,972:INFO:Importing untrained model
2025-11-11 00:50:53,972:INFO:SVM - Linear Kernel Imported successfully
2025-11-11 00:50:53,972:INFO:Starting cross validation
2025-11-11 00:50:53,972:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:50:54,568:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:54,578:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:54,587:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:54,619:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:54,621:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:54,625:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:54,625:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:54,646:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:54,646:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:54,651:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:54,674:INFO:Calculating mean and std
2025-11-11 00:50:54,675:INFO:Creating metrics dataframe
2025-11-11 00:50:54,677:INFO:Uploading results into container
2025-11-11 00:50:54,677:INFO:Uploading model into container now
2025-11-11 00:50:54,677:INFO:_master_model_container: 5
2025-11-11 00:50:54,677:INFO:_display_container: 2
2025-11-11 00:50:54,678:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-11-11 00:50:54,678:INFO:create_model() successfully completed......................................
2025-11-11 00:50:54,757:INFO:SubProcess create_model() end ==================================
2025-11-11 00:50:54,757:INFO:Creating metrics dataframe
2025-11-11 00:50:54,765:INFO:Initializing Ridge Classifier
2025-11-11 00:50:54,766:INFO:Total runtime is 0.20809114376703897 minutes
2025-11-11 00:50:54,766:INFO:SubProcess create_model() called ==================================
2025-11-11 00:50:54,766:INFO:Initializing create_model()
2025-11-11 00:50:54,766:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:50:54,766:INFO:Checking exceptions
2025-11-11 00:50:54,766:INFO:Importing libraries
2025-11-11 00:50:54,766:INFO:Copying training dataset
2025-11-11 00:50:54,776:INFO:Defining folds
2025-11-11 00:50:54,776:INFO:Declaring metric variables
2025-11-11 00:50:54,776:INFO:Importing untrained model
2025-11-11 00:50:54,776:INFO:Ridge Classifier Imported successfully
2025-11-11 00:50:54,776:INFO:Starting cross validation
2025-11-11 00:50:54,780:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:50:55,293:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:55,301:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:55,301:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:55,317:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:55,317:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:55,319:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:55,329:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:55,331:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:55,331:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:55,336:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:55,362:INFO:Calculating mean and std
2025-11-11 00:50:55,363:INFO:Creating metrics dataframe
2025-11-11 00:50:55,366:INFO:Uploading results into container
2025-11-11 00:50:55,366:INFO:Uploading model into container now
2025-11-11 00:50:55,366:INFO:_master_model_container: 6
2025-11-11 00:50:55,366:INFO:_display_container: 2
2025-11-11 00:50:55,366:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-11-11 00:50:55,366:INFO:create_model() successfully completed......................................
2025-11-11 00:50:55,449:INFO:SubProcess create_model() end ==================================
2025-11-11 00:50:55,449:INFO:Creating metrics dataframe
2025-11-11 00:50:55,452:INFO:Initializing Random Forest Classifier
2025-11-11 00:50:55,452:INFO:Total runtime is 0.21953216393788655 minutes
2025-11-11 00:50:55,452:INFO:SubProcess create_model() called ==================================
2025-11-11 00:50:55,452:INFO:Initializing create_model()
2025-11-11 00:50:55,452:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:50:55,452:INFO:Checking exceptions
2025-11-11 00:50:55,452:INFO:Importing libraries
2025-11-11 00:50:55,452:INFO:Copying training dataset
2025-11-11 00:50:55,465:INFO:Defining folds
2025-11-11 00:50:55,465:INFO:Declaring metric variables
2025-11-11 00:50:55,466:INFO:Importing untrained model
2025-11-11 00:50:55,466:INFO:Random Forest Classifier Imported successfully
2025-11-11 00:50:55,466:INFO:Starting cross validation
2025-11-11 00:50:55,468:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:50:57,186:INFO:Calculating mean and std
2025-11-11 00:50:57,186:INFO:Creating metrics dataframe
2025-11-11 00:50:57,188:INFO:Uploading results into container
2025-11-11 00:50:57,188:INFO:Uploading model into container now
2025-11-11 00:50:57,188:INFO:_master_model_container: 7
2025-11-11 00:50:57,188:INFO:_display_container: 2
2025-11-11 00:50:57,188:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-11-11 00:50:57,188:INFO:create_model() successfully completed......................................
2025-11-11 00:50:57,269:INFO:SubProcess create_model() end ==================================
2025-11-11 00:50:57,269:INFO:Creating metrics dataframe
2025-11-11 00:50:57,276:INFO:Initializing Quadratic Discriminant Analysis
2025-11-11 00:50:57,276:INFO:Total runtime is 0.24992300669352213 minutes
2025-11-11 00:50:57,276:INFO:SubProcess create_model() called ==================================
2025-11-11 00:50:57,276:INFO:Initializing create_model()
2025-11-11 00:50:57,276:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:50:57,276:INFO:Checking exceptions
2025-11-11 00:50:57,276:INFO:Importing libraries
2025-11-11 00:50:57,276:INFO:Copying training dataset
2025-11-11 00:50:57,287:INFO:Defining folds
2025-11-11 00:50:57,287:INFO:Declaring metric variables
2025-11-11 00:50:57,287:INFO:Importing untrained model
2025-11-11 00:50:57,287:INFO:Quadratic Discriminant Analysis Imported successfully
2025-11-11 00:50:57,287:INFO:Starting cross validation
2025-11-11 00:50:57,289:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:50:57,778:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-11 00:50:57,778:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-11 00:50:57,778:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-11 00:50:57,778:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-11 00:50:57,778:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-11 00:50:57,778:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-11 00:50:57,784:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-11 00:50:57,791:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-11 00:50:57,806:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-11 00:50:57,810:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:57,814:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:57,816:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:57,816:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:57,817:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:57,819:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:57,826:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:57,830:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-11 00:50:57,832:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:57,840:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:57,856:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:57,872:INFO:Calculating mean and std
2025-11-11 00:50:57,872:INFO:Creating metrics dataframe
2025-11-11 00:50:57,875:INFO:Uploading results into container
2025-11-11 00:50:57,876:INFO:Uploading model into container now
2025-11-11 00:50:57,876:INFO:_master_model_container: 8
2025-11-11 00:50:57,876:INFO:_display_container: 2
2025-11-11 00:50:57,876:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-11-11 00:50:57,877:INFO:create_model() successfully completed......................................
2025-11-11 00:50:57,951:INFO:SubProcess create_model() end ==================================
2025-11-11 00:50:57,955:INFO:Creating metrics dataframe
2025-11-11 00:50:57,957:INFO:Initializing Ada Boost Classifier
2025-11-11 00:50:57,957:INFO:Total runtime is 0.2612793723742167 minutes
2025-11-11 00:50:57,957:INFO:SubProcess create_model() called ==================================
2025-11-11 00:50:57,957:INFO:Initializing create_model()
2025-11-11 00:50:57,957:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:50:57,957:INFO:Checking exceptions
2025-11-11 00:50:57,957:INFO:Importing libraries
2025-11-11 00:50:57,957:INFO:Copying training dataset
2025-11-11 00:50:57,967:INFO:Defining folds
2025-11-11 00:50:57,967:INFO:Declaring metric variables
2025-11-11 00:50:57,967:INFO:Importing untrained model
2025-11-11 00:50:57,967:INFO:Ada Boost Classifier Imported successfully
2025-11-11 00:50:57,967:INFO:Starting cross validation
2025-11-11 00:50:57,967:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:50:58,415:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-11 00:50:58,437:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-11 00:50:58,437:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-11 00:50:58,441:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-11 00:50:58,449:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-11 00:50:58,469:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-11 00:50:58,478:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-11 00:50:58,480:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-11 00:50:58,489:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-11 00:50:58,507:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-11 00:50:59,288:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:59,298:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:59,309:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:59,325:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:59,332:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:59,343:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:59,347:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:59,350:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:59,352:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:59,363:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:50:59,386:INFO:Calculating mean and std
2025-11-11 00:50:59,388:INFO:Creating metrics dataframe
2025-11-11 00:50:59,388:INFO:Uploading results into container
2025-11-11 00:50:59,388:INFO:Uploading model into container now
2025-11-11 00:50:59,388:INFO:_master_model_container: 9
2025-11-11 00:50:59,388:INFO:_display_container: 2
2025-11-11 00:50:59,388:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2025-11-11 00:50:59,388:INFO:create_model() successfully completed......................................
2025-11-11 00:50:59,471:INFO:SubProcess create_model() end ==================================
2025-11-11 00:50:59,471:INFO:Creating metrics dataframe
2025-11-11 00:50:59,475:INFO:Initializing Gradient Boosting Classifier
2025-11-11 00:50:59,475:INFO:Total runtime is 0.28657697836558027 minutes
2025-11-11 00:50:59,475:INFO:SubProcess create_model() called ==================================
2025-11-11 00:50:59,476:INFO:Initializing create_model()
2025-11-11 00:50:59,476:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:50:59,476:INFO:Checking exceptions
2025-11-11 00:50:59,476:INFO:Importing libraries
2025-11-11 00:50:59,476:INFO:Copying training dataset
2025-11-11 00:50:59,487:INFO:Defining folds
2025-11-11 00:50:59,487:INFO:Declaring metric variables
2025-11-11 00:50:59,487:INFO:Importing untrained model
2025-11-11 00:50:59,487:INFO:Gradient Boosting Classifier Imported successfully
2025-11-11 00:50:59,489:INFO:Starting cross validation
2025-11-11 00:50:59,489:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:51:10,003:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,029:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,042:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,044:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,048:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,066:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,066:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,082:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,093:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,136:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,147:INFO:Calculating mean and std
2025-11-11 00:51:10,148:INFO:Creating metrics dataframe
2025-11-11 00:51:10,148:INFO:Uploading results into container
2025-11-11 00:51:10,148:INFO:Uploading model into container now
2025-11-11 00:51:10,148:INFO:_master_model_container: 10
2025-11-11 00:51:10,148:INFO:_display_container: 2
2025-11-11 00:51:10,148:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-11-11 00:51:10,148:INFO:create_model() successfully completed......................................
2025-11-11 00:51:10,232:INFO:SubProcess create_model() end ==================================
2025-11-11 00:51:10,234:INFO:Creating metrics dataframe
2025-11-11 00:51:10,236:INFO:Initializing Linear Discriminant Analysis
2025-11-11 00:51:10,237:INFO:Total runtime is 0.465940546989441 minutes
2025-11-11 00:51:10,237:INFO:SubProcess create_model() called ==================================
2025-11-11 00:51:10,237:INFO:Initializing create_model()
2025-11-11 00:51:10,237:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:51:10,237:INFO:Checking exceptions
2025-11-11 00:51:10,237:INFO:Importing libraries
2025-11-11 00:51:10,237:INFO:Copying training dataset
2025-11-11 00:51:10,247:INFO:Defining folds
2025-11-11 00:51:10,247:INFO:Declaring metric variables
2025-11-11 00:51:10,247:INFO:Importing untrained model
2025-11-11 00:51:10,247:INFO:Linear Discriminant Analysis Imported successfully
2025-11-11 00:51:10,247:INFO:Starting cross validation
2025-11-11 00:51:10,247:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:51:10,775:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,776:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,783:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,786:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,791:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,799:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,808:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,817:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,817:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,827:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-11 00:51:10,846:INFO:Calculating mean and std
2025-11-11 00:51:10,846:INFO:Creating metrics dataframe
2025-11-11 00:51:10,849:INFO:Uploading results into container
2025-11-11 00:51:10,849:INFO:Uploading model into container now
2025-11-11 00:51:10,849:INFO:_master_model_container: 11
2025-11-11 00:51:10,849:INFO:_display_container: 2
2025-11-11 00:51:10,849:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-11-11 00:51:10,849:INFO:create_model() successfully completed......................................
2025-11-11 00:51:10,928:INFO:SubProcess create_model() end ==================================
2025-11-11 00:51:10,928:INFO:Creating metrics dataframe
2025-11-11 00:51:10,932:INFO:Initializing Extra Trees Classifier
2025-11-11 00:51:10,932:INFO:Total runtime is 0.47752162615458177 minutes
2025-11-11 00:51:10,932:INFO:SubProcess create_model() called ==================================
2025-11-11 00:51:10,932:INFO:Initializing create_model()
2025-11-11 00:51:10,932:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:51:10,932:INFO:Checking exceptions
2025-11-11 00:51:10,932:INFO:Importing libraries
2025-11-11 00:51:10,932:INFO:Copying training dataset
2025-11-11 00:51:10,947:INFO:Defining folds
2025-11-11 00:51:10,947:INFO:Declaring metric variables
2025-11-11 00:51:10,947:INFO:Importing untrained model
2025-11-11 00:51:10,947:INFO:Extra Trees Classifier Imported successfully
2025-11-11 00:51:10,947:INFO:Starting cross validation
2025-11-11 00:51:10,949:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:51:12,182:INFO:Calculating mean and std
2025-11-11 00:51:12,182:INFO:Creating metrics dataframe
2025-11-11 00:51:12,182:INFO:Uploading results into container
2025-11-11 00:51:12,185:INFO:Uploading model into container now
2025-11-11 00:51:12,186:INFO:_master_model_container: 12
2025-11-11 00:51:12,186:INFO:_display_container: 2
2025-11-11 00:51:12,186:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-11-11 00:51:12,186:INFO:create_model() successfully completed......................................
2025-11-11 00:51:12,280:INFO:SubProcess create_model() end ==================================
2025-11-11 00:51:12,280:INFO:Creating metrics dataframe
2025-11-11 00:51:12,283:INFO:Initializing Extreme Gradient Boosting
2025-11-11 00:51:12,283:INFO:Total runtime is 0.5000518997510275 minutes
2025-11-11 00:51:12,283:INFO:SubProcess create_model() called ==================================
2025-11-11 00:51:12,283:INFO:Initializing create_model()
2025-11-11 00:51:12,283:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:51:12,283:INFO:Checking exceptions
2025-11-11 00:51:12,283:INFO:Importing libraries
2025-11-11 00:51:12,283:INFO:Copying training dataset
2025-11-11 00:51:12,300:INFO:Defining folds
2025-11-11 00:51:12,300:INFO:Declaring metric variables
2025-11-11 00:51:12,300:INFO:Importing untrained model
2025-11-11 00:51:12,300:INFO:Extreme Gradient Boosting Imported successfully
2025-11-11 00:51:12,300:INFO:Starting cross validation
2025-11-11 00:51:12,300:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:51:17,303:INFO:Calculating mean and std
2025-11-11 00:51:17,305:INFO:Creating metrics dataframe
2025-11-11 00:51:17,306:INFO:Uploading results into container
2025-11-11 00:51:17,306:INFO:Uploading model into container now
2025-11-11 00:51:17,308:INFO:_master_model_container: 13
2025-11-11 00:51:17,308:INFO:_display_container: 2
2025-11-11 00:51:17,308:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-11-11 00:51:17,308:INFO:create_model() successfully completed......................................
2025-11-11 00:51:17,389:INFO:SubProcess create_model() end ==================================
2025-11-11 00:51:17,391:INFO:Creating metrics dataframe
2025-11-11 00:51:17,393:INFO:Initializing Light Gradient Boosting Machine
2025-11-11 00:51:17,393:INFO:Total runtime is 0.5852058251698813 minutes
2025-11-11 00:51:17,394:INFO:SubProcess create_model() called ==================================
2025-11-11 00:51:17,394:INFO:Initializing create_model()
2025-11-11 00:51:17,394:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:51:17,394:INFO:Checking exceptions
2025-11-11 00:51:17,394:INFO:Importing libraries
2025-11-11 00:51:17,394:INFO:Copying training dataset
2025-11-11 00:51:17,406:INFO:Defining folds
2025-11-11 00:51:17,406:INFO:Declaring metric variables
2025-11-11 00:51:17,406:INFO:Importing untrained model
2025-11-11 00:51:17,407:INFO:Light Gradient Boosting Machine Imported successfully
2025-11-11 00:51:17,407:INFO:Starting cross validation
2025-11-11 00:51:17,408:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:51:23,160:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-11 00:51:23,165:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-11 00:51:23,176:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-11 00:51:23,180:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-11 00:51:23,185:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-11 00:51:23,227:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-11 00:51:23,236:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-11 00:51:23,245:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-11 00:51:23,258:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-11 00:51:23,268:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-11 00:51:23,284:INFO:Calculating mean and std
2025-11-11 00:51:23,286:INFO:Creating metrics dataframe
2025-11-11 00:51:23,287:INFO:Uploading results into container
2025-11-11 00:51:23,287:INFO:Uploading model into container now
2025-11-11 00:51:23,289:INFO:_master_model_container: 14
2025-11-11 00:51:23,289:INFO:_display_container: 2
2025-11-11 00:51:23,289:INFO:<lightgbm.LGBMClassifier object at 0x0000015F471AF430>
2025-11-11 00:51:23,289:INFO:create_model() successfully completed......................................
2025-11-11 00:51:23,375:INFO:SubProcess create_model() end ==================================
2025-11-11 00:51:23,375:INFO:Creating metrics dataframe
2025-11-11 00:51:23,377:INFO:Initializing Dummy Classifier
2025-11-11 00:51:23,377:INFO:Total runtime is 0.6849486072858175 minutes
2025-11-11 00:51:23,377:INFO:SubProcess create_model() called ==================================
2025-11-11 00:51:23,377:INFO:Initializing create_model()
2025-11-11 00:51:23,377:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000015F46EDCCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:51:23,377:INFO:Checking exceptions
2025-11-11 00:51:23,377:INFO:Importing libraries
2025-11-11 00:51:23,377:INFO:Copying training dataset
2025-11-11 00:51:23,386:INFO:Defining folds
2025-11-11 00:51:23,390:INFO:Declaring metric variables
2025-11-11 00:51:23,390:INFO:Importing untrained model
2025-11-11 00:51:23,390:INFO:Dummy Classifier Imported successfully
2025-11-11 00:51:23,390:INFO:Starting cross validation
2025-11-11 00:51:23,395:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-11 00:51:23,917:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-11 00:51:23,933:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-11 00:51:23,978:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-11 00:51:23,986:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-11 00:51:23,988:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-11 00:51:23,988:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-11 00:51:23,996:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-11 00:51:23,996:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-11 00:51:24,007:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-11 00:51:24,031:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-11 00:51:24,045:INFO:Calculating mean and std
2025-11-11 00:51:24,045:INFO:Creating metrics dataframe
2025-11-11 00:51:24,047:INFO:Uploading results into container
2025-11-11 00:51:24,047:INFO:Uploading model into container now
2025-11-11 00:51:24,047:INFO:_master_model_container: 15
2025-11-11 00:51:24,047:INFO:_display_container: 2
2025-11-11 00:51:24,047:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2025-11-11 00:51:24,047:INFO:create_model() successfully completed......................................
2025-11-11 00:51:24,133:INFO:SubProcess create_model() end ==================================
2025-11-11 00:51:24,133:INFO:Creating metrics dataframe
2025-11-11 00:51:24,138:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-11-11 00:51:24,139:INFO:Initializing create_model()
2025-11-11 00:51:24,139:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:51:24,141:INFO:Checking exceptions
2025-11-11 00:51:24,141:INFO:Importing libraries
2025-11-11 00:51:24,141:INFO:Copying training dataset
2025-11-11 00:51:24,149:INFO:Defining folds
2025-11-11 00:51:24,149:INFO:Declaring metric variables
2025-11-11 00:51:24,149:INFO:Importing untrained model
2025-11-11 00:51:24,149:INFO:Declaring custom model
2025-11-11 00:51:24,149:INFO:Gradient Boosting Classifier Imported successfully
2025-11-11 00:51:24,149:INFO:Cross validation set to False
2025-11-11 00:51:24,149:INFO:Fitting Model
2025-11-11 00:51:31,687:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-11-11 00:51:31,687:INFO:create_model() successfully completed......................................
2025-11-11 00:51:31,826:INFO:_master_model_container: 15
2025-11-11 00:51:31,826:INFO:_display_container: 2
2025-11-11 00:51:31,827:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-11-11 00:51:31,827:INFO:compare_models() successfully completed......................................
2025-11-11 00:51:31,827:INFO:Initializing finalize_model()
2025-11-11 00:51:31,827:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-11-11 00:51:31,827:INFO:Finalizing GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-11-11 00:51:31,836:INFO:Initializing create_model()
2025-11-11 00:51:31,839:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-11-11 00:51:31,839:INFO:Checking exceptions
2025-11-11 00:51:31,840:INFO:Importing libraries
2025-11-11 00:51:31,840:INFO:Copying training dataset
2025-11-11 00:51:31,841:INFO:Defining folds
2025-11-11 00:51:31,841:INFO:Declaring metric variables
2025-11-11 00:51:31,841:INFO:Importing untrained model
2025-11-11 00:51:31,841:INFO:Declaring custom model
2025-11-11 00:51:31,841:INFO:Gradient Boosting Classifier Imported successfully
2025-11-11 00:51:31,847:INFO:Cross validation set to False
2025-11-11 00:51:31,847:INFO:Fitting Model
2025-11-11 00:51:41,629:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2025-11-11 00:51:41,635:INFO:create_model() successfully completed......................................
2025-11-11 00:51:41,756:INFO:_master_model_container: 15
2025-11-11 00:51:41,756:INFO:_display_container: 2
2025-11-11 00:51:41,759:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2025-11-11 00:51:41,759:INFO:finalize_model() successfully completed......................................
2025-11-11 00:51:41,849:INFO:Initializing predict_model()
2025-11-11 00:51:41,849:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000015F46CB1130>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000015F47134E50>)
2025-11-11 00:51:41,849:INFO:Checking exceptions
2025-11-11 00:51:41,849:INFO:Preloading libraries
2025-11-11 00:51:42,117:INFO:Initializing save_model()
2025-11-11 00:51:42,124:INFO:save_model(model=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False), model_name=reports\phase3\pycaret\best_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\maraw\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_fa...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-11-11 00:51:42,124:INFO:Adding model into prep_pipe
2025-11-11 00:51:42,124:WARNING:Only Model saved as it was a pipeline.
2025-11-11 00:51:42,139:INFO:reports\phase3\pycaret\best_model.pkl saved in current working directory
2025-11-11 00:51:42,145:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2025-11-11 00:51:42,145:INFO:save_model() successfully completed......................................
2025-11-15 23:31:48,776:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-15 23:31:48,777:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-15 23:31:48,777:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-15 23:31:48,777:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-11-15 23:31:50,267:INFO:PyCaret ClassificationExperiment
2025-11-15 23:31:50,267:INFO:Logging name: clf-default-name
2025-11-15 23:31:50,267:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-11-15 23:31:50,267:INFO:version 3.3.2
2025-11-15 23:31:50,267:INFO:Initializing setup()
2025-11-15 23:31:50,267:INFO:self.USI: e791
2025-11-15 23:31:50,267:INFO:self._variable_keys: {'is_multiclass', 'USI', 'y_train', 'seed', 'target_param', 'idx', 'X_train', 'memory', 'logging_param', 'fold_generator', 'gpu_n_jobs_param', 'fix_imbalance', 'y', 'data', 'gpu_param', '_available_plots', '_ml_usecase', 'exp_id', 'html_param', 'fold_shuffle_param', 'fold_groups_param', 'X_test', 'n_jobs_param', 'exp_name_log', 'log_plots_param', 'y_test', 'X', 'pipeline'}
2025-11-15 23:31:50,267:INFO:Checking environment
2025-11-15 23:31:50,267:INFO:python_version: 3.9.21
2025-11-15 23:31:50,267:INFO:python_build: ('main', 'Dec 11 2024 16:35:24')
2025-11-15 23:31:50,267:INFO:machine: AMD64
2025-11-15 23:31:50,277:INFO:platform: Windows-10-10.0.26200-SP0
2025-11-15 23:31:50,277:INFO:Memory: svmem(total=7897935872, available=1011990528, percent=87.2, used=6885945344, free=1011990528)
2025-11-15 23:31:50,277:INFO:Physical Core: 6
2025-11-15 23:31:50,278:INFO:Logical Core: 12
2025-11-15 23:31:50,278:INFO:Checking libraries
2025-11-15 23:31:50,278:INFO:System:
2025-11-15 23:31:50,278:INFO:    python: 3.9.21 (main, Dec 11 2024, 16:35:24) [MSC v.1929 64 bit (AMD64)]
2025-11-15 23:31:50,278:INFO:executable: C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\Scripts\python.exe
2025-11-15 23:31:50,278:INFO:   machine: Windows-10-10.0.26200-SP0
2025-11-15 23:31:50,278:INFO:PyCaret required dependencies:
2025-11-15 23:31:50,283:INFO:                 pip: 25.3
2025-11-15 23:31:50,283:INFO:          setuptools: 58.1.0
2025-11-15 23:31:50,283:INFO:             pycaret: 3.3.2
2025-11-15 23:31:50,283:INFO:             IPython: 8.18.1
2025-11-15 23:31:50,283:INFO:          ipywidgets: 8.1.8
2025-11-15 23:31:50,283:INFO:                tqdm: 4.67.1
2025-11-15 23:31:50,283:INFO:               numpy: 1.26.4
2025-11-15 23:31:50,283:INFO:              pandas: 2.1.4
2025-11-15 23:31:50,283:INFO:              jinja2: 3.1.6
2025-11-15 23:31:50,283:INFO:               scipy: 1.11.4
2025-11-15 23:31:50,283:INFO:              joblib: 1.3.2
2025-11-15 23:31:50,283:INFO:             sklearn: 1.4.2
2025-11-15 23:31:50,283:INFO:                pyod: 2.0.5
2025-11-15 23:31:50,283:INFO:            imblearn: 0.12.4
2025-11-15 23:31:50,283:INFO:   category_encoders: 2.6.4
2025-11-15 23:31:50,283:INFO:            lightgbm: Installed but version unavailable
2025-11-15 23:31:50,283:INFO:               numba: 0.60.0
2025-11-15 23:31:50,283:INFO:            requests: 2.32.5
2025-11-15 23:31:50,283:INFO:          matplotlib: 3.7.5
2025-11-15 23:31:50,283:INFO:          scikitplot: 0.3.7
2025-11-15 23:31:50,284:INFO:         yellowbrick: 1.5
2025-11-15 23:31:50,284:INFO:              plotly: 6.4.0
2025-11-15 23:31:50,284:INFO:    plotly-resampler: Not installed
2025-11-15 23:31:50,284:INFO:             kaleido: 1.2.0
2025-11-15 23:31:50,284:INFO:           schemdraw: 0.15
2025-11-15 23:31:50,284:INFO:         statsmodels: 0.14.5
2025-11-15 23:31:50,284:INFO:              sktime: 0.26.0
2025-11-15 23:31:50,284:INFO:               tbats: 1.1.3
2025-11-15 23:31:50,284:INFO:            pmdarima: 2.0.4
2025-11-15 23:31:50,284:INFO:              psutil: 7.1.3
2025-11-15 23:31:50,284:INFO:          markupsafe: 3.0.3
2025-11-15 23:31:50,284:INFO:             pickle5: Not installed
2025-11-15 23:31:50,284:INFO:         cloudpickle: 3.1.2
2025-11-15 23:31:50,284:INFO:         deprecation: 2.1.0
2025-11-15 23:31:50,284:INFO:              xxhash: 3.6.0
2025-11-15 23:31:50,284:INFO:           wurlitzer: Not installed
2025-11-15 23:31:50,284:INFO:PyCaret optional dependencies:
2025-11-15 23:31:50,323:INFO:                shap: Not installed
2025-11-15 23:31:50,323:INFO:           interpret: Not installed
2025-11-15 23:31:50,323:INFO:                umap: 0.5.4
2025-11-15 23:31:50,323:INFO:     ydata_profiling: Not installed
2025-11-15 23:31:50,323:INFO:  explainerdashboard: Not installed
2025-11-15 23:31:50,323:INFO:             autoviz: Not installed
2025-11-15 23:31:50,323:INFO:           fairlearn: Not installed
2025-11-15 23:31:50,323:INFO:          deepchecks: Not installed
2025-11-15 23:31:50,323:INFO:             xgboost: 2.0.3
2025-11-15 23:31:50,323:INFO:            catboost: Not installed
2025-11-15 23:31:50,323:INFO:              kmodes: Not installed
2025-11-15 23:31:50,323:INFO:             mlxtend: Not installed
2025-11-15 23:31:50,323:INFO:       statsforecast: Not installed
2025-11-15 23:31:50,323:INFO:        tune_sklearn: Not installed
2025-11-15 23:31:50,323:INFO:                 ray: Not installed
2025-11-15 23:31:50,323:INFO:            hyperopt: Not installed
2025-11-15 23:31:50,323:INFO:              optuna: 3.6.1
2025-11-15 23:31:50,323:INFO:               skopt: Not installed
2025-11-15 23:31:50,323:INFO:              mlflow: Not installed
2025-11-15 23:31:50,323:INFO:              gradio: Not installed
2025-11-15 23:31:50,323:INFO:             fastapi: Not installed
2025-11-15 23:31:50,323:INFO:             uvicorn: Not installed
2025-11-15 23:31:50,323:INFO:              m2cgen: Not installed
2025-11-15 23:31:50,323:INFO:           evidently: Not installed
2025-11-15 23:31:50,323:INFO:               fugue: Not installed
2025-11-15 23:31:50,323:INFO:           streamlit: Not installed
2025-11-15 23:31:50,323:INFO:             prophet: Not installed
2025-11-15 23:31:50,324:INFO:None
2025-11-15 23:31:50,324:INFO:Set up data.
2025-11-15 23:31:50,339:INFO:Set up folding strategy.
2025-11-15 23:31:50,339:INFO:Set up train/test split.
2025-11-15 23:31:50,350:INFO:Set up index.
2025-11-15 23:31:50,352:INFO:Assigning column types.
2025-11-15 23:31:50,361:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-11-15 23:31:50,396:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-11-15 23:31:50,400:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-11-15 23:31:50,429:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-15 23:31:50,431:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-15 23:31:50,465:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-11-15 23:31:50,465:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-11-15 23:31:50,487:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-15 23:31:50,489:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-15 23:31:50,489:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-11-15 23:31:50,523:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-11-15 23:31:50,544:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-15 23:31:50,546:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-15 23:31:50,580:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-11-15 23:31:50,603:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-15 23:31:50,605:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-15 23:31:50,605:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-11-15 23:31:50,663:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-15 23:31:50,665:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-15 23:31:50,723:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-15 23:31:50,726:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-15 23:31:50,742:INFO:Preparing preprocessing pipeline...
2025-11-15 23:31:50,745:INFO:Set up simple imputation.
2025-11-15 23:31:50,745:INFO:Set up column transformation.
2025-11-15 23:31:50,745:INFO:Set up feature normalization.
2025-11-15 23:31:50,830:INFO:Finished creating preprocessing pipeline.
2025-11-15 23:31:50,835:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\maraw\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_fa...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2025-11-15 23:31:50,836:INFO:Creating final display dataframe.
2025-11-15 23:31:51,025:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target          label_id
2                   Target type        Multiclass
3           Original data shape        (1641, 79)
4        Transformed data shape        (1641, 79)
5   Transformed train set shape        (1312, 79)
6    Transformed test set shape         (329, 79)
7              Numeric features                78
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Transformation              True
13        Transformation method       yeo-johnson
14                    Normalize              True
15             Normalize method            zscore
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              e791
2025-11-15 23:31:51,083:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-15 23:31:51,084:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-15 23:31:51,141:INFO:Soft dependency imported: xgboost: 2.0.3
2025-11-15 23:31:51,143:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-11-15 23:31:51,144:INFO:setup() successfully completed in 0.88s...............
2025-11-15 23:31:51,144:INFO:Initializing compare_models()
2025-11-15 23:31:51,144:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-11-15 23:31:51,144:INFO:Checking exceptions
2025-11-15 23:31:51,154:INFO:Preparing display monitor
2025-11-15 23:31:51,158:INFO:Initializing Logistic Regression
2025-11-15 23:31:51,158:INFO:Total runtime is 0.0 minutes
2025-11-15 23:31:51,158:INFO:SubProcess create_model() called ==================================
2025-11-15 23:31:51,158:INFO:Initializing create_model()
2025-11-15 23:31:51,158:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:31:51,159:INFO:Checking exceptions
2025-11-15 23:31:51,159:INFO:Importing libraries
2025-11-15 23:31:51,159:INFO:Copying training dataset
2025-11-15 23:31:51,169:INFO:Defining folds
2025-11-15 23:31:51,169:INFO:Declaring metric variables
2025-11-15 23:31:51,169:INFO:Importing untrained model
2025-11-15 23:31:51,169:INFO:Logistic Regression Imported successfully
2025-11-15 23:31:51,170:INFO:Starting cross validation
2025-11-15 23:31:51,171:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:31:57,710:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:31:57,748:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:31:57,759:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:31:57,764:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:31:57,776:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:31:57,779:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:31:57,797:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:31:57,843:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:31:57,850:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:31:57,893:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:31:57,913:INFO:Calculating mean and std
2025-11-15 23:31:57,915:INFO:Creating metrics dataframe
2025-11-15 23:31:57,918:INFO:Uploading results into container
2025-11-15 23:31:57,920:INFO:Uploading model into container now
2025-11-15 23:31:57,920:INFO:_master_model_container: 1
2025-11-15 23:31:57,920:INFO:_display_container: 2
2025-11-15 23:31:57,920:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-11-15 23:31:57,920:INFO:create_model() successfully completed......................................
2025-11-15 23:31:58,040:INFO:SubProcess create_model() end ==================================
2025-11-15 23:31:58,040:INFO:Creating metrics dataframe
2025-11-15 23:31:58,047:INFO:Initializing K Neighbors Classifier
2025-11-15 23:31:58,048:INFO:Total runtime is 0.11483375231424968 minutes
2025-11-15 23:31:58,048:INFO:SubProcess create_model() called ==================================
2025-11-15 23:31:58,048:INFO:Initializing create_model()
2025-11-15 23:31:58,048:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:31:58,048:INFO:Checking exceptions
2025-11-15 23:31:58,048:INFO:Importing libraries
2025-11-15 23:31:58,048:INFO:Copying training dataset
2025-11-15 23:31:58,066:INFO:Defining folds
2025-11-15 23:31:58,066:INFO:Declaring metric variables
2025-11-15 23:31:58,066:INFO:Importing untrained model
2025-11-15 23:31:58,067:INFO:K Neighbors Classifier Imported successfully
2025-11-15 23:31:58,067:INFO:Starting cross validation
2025-11-15 23:31:58,068:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:02,054:INFO:Calculating mean and std
2025-11-15 23:32:02,055:INFO:Creating metrics dataframe
2025-11-15 23:32:02,057:INFO:Uploading results into container
2025-11-15 23:32:02,057:INFO:Uploading model into container now
2025-11-15 23:32:02,058:INFO:_master_model_container: 2
2025-11-15 23:32:02,058:INFO:_display_container: 2
2025-11-15 23:32:02,058:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-11-15 23:32:02,058:INFO:create_model() successfully completed......................................
2025-11-15 23:32:02,152:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:02,152:INFO:Creating metrics dataframe
2025-11-15 23:32:02,154:INFO:Initializing Naive Bayes
2025-11-15 23:32:02,154:INFO:Total runtime is 0.18327502806981405 minutes
2025-11-15 23:32:02,155:INFO:SubProcess create_model() called ==================================
2025-11-15 23:32:02,155:INFO:Initializing create_model()
2025-11-15 23:32:02,155:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:02,155:INFO:Checking exceptions
2025-11-15 23:32:02,155:INFO:Importing libraries
2025-11-15 23:32:02,155:INFO:Copying training dataset
2025-11-15 23:32:02,166:INFO:Defining folds
2025-11-15 23:32:02,166:INFO:Declaring metric variables
2025-11-15 23:32:02,167:INFO:Importing untrained model
2025-11-15 23:32:02,167:INFO:Naive Bayes Imported successfully
2025-11-15 23:32:02,167:INFO:Starting cross validation
2025-11-15 23:32:02,168:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:02,862:INFO:Calculating mean and std
2025-11-15 23:32:02,863:INFO:Creating metrics dataframe
2025-11-15 23:32:02,864:INFO:Uploading results into container
2025-11-15 23:32:02,865:INFO:Uploading model into container now
2025-11-15 23:32:02,865:INFO:_master_model_container: 3
2025-11-15 23:32:02,865:INFO:_display_container: 2
2025-11-15 23:32:02,865:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-11-15 23:32:02,866:INFO:create_model() successfully completed......................................
2025-11-15 23:32:02,953:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:02,953:INFO:Creating metrics dataframe
2025-11-15 23:32:02,955:INFO:Initializing Decision Tree Classifier
2025-11-15 23:32:02,955:INFO:Total runtime is 0.19661768277486166 minutes
2025-11-15 23:32:02,956:INFO:SubProcess create_model() called ==================================
2025-11-15 23:32:02,956:INFO:Initializing create_model()
2025-11-15 23:32:02,956:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:02,956:INFO:Checking exceptions
2025-11-15 23:32:02,956:INFO:Importing libraries
2025-11-15 23:32:02,956:INFO:Copying training dataset
2025-11-15 23:32:02,966:INFO:Defining folds
2025-11-15 23:32:02,966:INFO:Declaring metric variables
2025-11-15 23:32:02,967:INFO:Importing untrained model
2025-11-15 23:32:02,967:INFO:Decision Tree Classifier Imported successfully
2025-11-15 23:32:02,967:INFO:Starting cross validation
2025-11-15 23:32:02,968:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:03,735:INFO:Calculating mean and std
2025-11-15 23:32:03,736:INFO:Creating metrics dataframe
2025-11-15 23:32:03,738:INFO:Uploading results into container
2025-11-15 23:32:03,739:INFO:Uploading model into container now
2025-11-15 23:32:03,739:INFO:_master_model_container: 4
2025-11-15 23:32:03,739:INFO:_display_container: 2
2025-11-15 23:32:03,739:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2025-11-15 23:32:03,739:INFO:create_model() successfully completed......................................
2025-11-15 23:32:03,824:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:03,824:INFO:Creating metrics dataframe
2025-11-15 23:32:03,826:INFO:Initializing SVM - Linear Kernel
2025-11-15 23:32:03,826:INFO:Total runtime is 0.21114657719930013 minutes
2025-11-15 23:32:03,826:INFO:SubProcess create_model() called ==================================
2025-11-15 23:32:03,826:INFO:Initializing create_model()
2025-11-15 23:32:03,826:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:03,826:INFO:Checking exceptions
2025-11-15 23:32:03,826:INFO:Importing libraries
2025-11-15 23:32:03,826:INFO:Copying training dataset
2025-11-15 23:32:03,837:INFO:Defining folds
2025-11-15 23:32:03,837:INFO:Declaring metric variables
2025-11-15 23:32:03,837:INFO:Importing untrained model
2025-11-15 23:32:03,837:INFO:SVM - Linear Kernel Imported successfully
2025-11-15 23:32:03,837:INFO:Starting cross validation
2025-11-15 23:32:03,838:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:04,504:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:04,506:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:04,515:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:04,529:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:04,539:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:04,546:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:04,549:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:04,551:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:04,560:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:04,587:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:04,605:INFO:Calculating mean and std
2025-11-15 23:32:04,607:INFO:Creating metrics dataframe
2025-11-15 23:32:04,608:INFO:Uploading results into container
2025-11-15 23:32:04,609:INFO:Uploading model into container now
2025-11-15 23:32:04,609:INFO:_master_model_container: 5
2025-11-15 23:32:04,609:INFO:_display_container: 2
2025-11-15 23:32:04,609:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-11-15 23:32:04,609:INFO:create_model() successfully completed......................................
2025-11-15 23:32:04,692:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:04,692:INFO:Creating metrics dataframe
2025-11-15 23:32:04,694:INFO:Initializing Ridge Classifier
2025-11-15 23:32:04,694:INFO:Total runtime is 0.22560295661290486 minutes
2025-11-15 23:32:04,694:INFO:SubProcess create_model() called ==================================
2025-11-15 23:32:04,694:INFO:Initializing create_model()
2025-11-15 23:32:04,694:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:04,694:INFO:Checking exceptions
2025-11-15 23:32:04,694:INFO:Importing libraries
2025-11-15 23:32:04,694:INFO:Copying training dataset
2025-11-15 23:32:04,704:INFO:Defining folds
2025-11-15 23:32:04,704:INFO:Declaring metric variables
2025-11-15 23:32:04,704:INFO:Importing untrained model
2025-11-15 23:32:04,705:INFO:Ridge Classifier Imported successfully
2025-11-15 23:32:04,705:INFO:Starting cross validation
2025-11-15 23:32:04,706:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:05,289:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:05,291:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:05,292:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:05,299:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:05,303:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:05,304:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:05,306:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:05,306:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:05,309:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:05,315:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:05,335:INFO:Calculating mean and std
2025-11-15 23:32:05,336:INFO:Creating metrics dataframe
2025-11-15 23:32:05,339:INFO:Uploading results into container
2025-11-15 23:32:05,339:INFO:Uploading model into container now
2025-11-15 23:32:05,339:INFO:_master_model_container: 6
2025-11-15 23:32:05,339:INFO:_display_container: 2
2025-11-15 23:32:05,339:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2025-11-15 23:32:05,339:INFO:create_model() successfully completed......................................
2025-11-15 23:32:05,422:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:05,422:INFO:Creating metrics dataframe
2025-11-15 23:32:05,424:INFO:Initializing Random Forest Classifier
2025-11-15 23:32:05,424:INFO:Total runtime is 0.2377803921699524 minutes
2025-11-15 23:32:05,425:INFO:SubProcess create_model() called ==================================
2025-11-15 23:32:05,425:INFO:Initializing create_model()
2025-11-15 23:32:05,425:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:05,425:INFO:Checking exceptions
2025-11-15 23:32:05,425:INFO:Importing libraries
2025-11-15 23:32:05,425:INFO:Copying training dataset
2025-11-15 23:32:05,437:INFO:Defining folds
2025-11-15 23:32:05,437:INFO:Declaring metric variables
2025-11-15 23:32:05,438:INFO:Importing untrained model
2025-11-15 23:32:05,438:INFO:Random Forest Classifier Imported successfully
2025-11-15 23:32:05,439:INFO:Starting cross validation
2025-11-15 23:32:05,439:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:07,339:INFO:Calculating mean and std
2025-11-15 23:32:07,340:INFO:Creating metrics dataframe
2025-11-15 23:32:07,341:INFO:Uploading results into container
2025-11-15 23:32:07,341:INFO:Uploading model into container now
2025-11-15 23:32:07,342:INFO:_master_model_container: 7
2025-11-15 23:32:07,342:INFO:_display_container: 2
2025-11-15 23:32:07,342:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2025-11-15 23:32:07,342:INFO:create_model() successfully completed......................................
2025-11-15 23:32:07,424:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:07,424:INFO:Creating metrics dataframe
2025-11-15 23:32:07,426:INFO:Initializing Quadratic Discriminant Analysis
2025-11-15 23:32:07,426:INFO:Total runtime is 0.27113399902979535 minutes
2025-11-15 23:32:07,426:INFO:SubProcess create_model() called ==================================
2025-11-15 23:32:07,427:INFO:Initializing create_model()
2025-11-15 23:32:07,427:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:07,427:INFO:Checking exceptions
2025-11-15 23:32:07,427:INFO:Importing libraries
2025-11-15 23:32:07,427:INFO:Copying training dataset
2025-11-15 23:32:07,436:INFO:Defining folds
2025-11-15 23:32:07,436:INFO:Declaring metric variables
2025-11-15 23:32:07,436:INFO:Importing untrained model
2025-11-15 23:32:07,436:INFO:Quadratic Discriminant Analysis Imported successfully
2025-11-15 23:32:07,437:INFO:Starting cross validation
2025-11-15 23:32:07,437:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:08,000:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-15 23:32:08,000:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-15 23:32:08,001:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-15 23:32:08,004:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-15 23:32:08,007:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-15 23:32:08,010:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-15 23:32:08,015:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-15 23:32:08,022:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-15 23:32:08,039:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2025-11-15 23:32:08,042:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:08,043:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:08,043:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:08,047:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:08,049:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:08,049:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:08,055:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:08,060:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:08,066:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:08,079:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:08,095:INFO:Calculating mean and std
2025-11-15 23:32:08,095:INFO:Creating metrics dataframe
2025-11-15 23:32:08,097:INFO:Uploading results into container
2025-11-15 23:32:08,097:INFO:Uploading model into container now
2025-11-15 23:32:08,098:INFO:_master_model_container: 8
2025-11-15 23:32:08,098:INFO:_display_container: 2
2025-11-15 23:32:08,098:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-11-15 23:32:08,098:INFO:create_model() successfully completed......................................
2025-11-15 23:32:08,186:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:08,186:INFO:Creating metrics dataframe
2025-11-15 23:32:08,188:INFO:Initializing Ada Boost Classifier
2025-11-15 23:32:08,188:INFO:Total runtime is 0.2838345090548198 minutes
2025-11-15 23:32:08,188:INFO:SubProcess create_model() called ==================================
2025-11-15 23:32:08,189:INFO:Initializing create_model()
2025-11-15 23:32:08,189:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:08,189:INFO:Checking exceptions
2025-11-15 23:32:08,189:INFO:Importing libraries
2025-11-15 23:32:08,189:INFO:Copying training dataset
2025-11-15 23:32:08,198:INFO:Defining folds
2025-11-15 23:32:08,198:INFO:Declaring metric variables
2025-11-15 23:32:08,198:INFO:Importing untrained model
2025-11-15 23:32:08,198:INFO:Ada Boost Classifier Imported successfully
2025-11-15 23:32:08,198:INFO:Starting cross validation
2025-11-15 23:32:08,200:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:08,754:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-15 23:32:08,757:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-15 23:32:08,769:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-15 23:32:08,773:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-15 23:32:08,774:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-15 23:32:08,775:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-15 23:32:08,783:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-15 23:32:08,785:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-15 23:32:08,798:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-15 23:32:08,803:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-11-15 23:32:09,709:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:09,713:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:09,728:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:09,731:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:09,739:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:09,751:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:09,753:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:09,753:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:09,753:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:09,754:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:09,772:INFO:Calculating mean and std
2025-11-15 23:32:09,773:INFO:Creating metrics dataframe
2025-11-15 23:32:09,775:INFO:Uploading results into container
2025-11-15 23:32:09,775:INFO:Uploading model into container now
2025-11-15 23:32:09,775:INFO:_master_model_container: 9
2025-11-15 23:32:09,775:INFO:_display_container: 2
2025-11-15 23:32:09,776:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2025-11-15 23:32:09,776:INFO:create_model() successfully completed......................................
2025-11-15 23:32:09,863:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:09,864:INFO:Creating metrics dataframe
2025-11-15 23:32:09,866:INFO:Initializing Gradient Boosting Classifier
2025-11-15 23:32:09,866:INFO:Total runtime is 0.31180852651596075 minutes
2025-11-15 23:32:09,866:INFO:SubProcess create_model() called ==================================
2025-11-15 23:32:09,867:INFO:Initializing create_model()
2025-11-15 23:32:09,867:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:09,867:INFO:Checking exceptions
2025-11-15 23:32:09,867:INFO:Importing libraries
2025-11-15 23:32:09,867:INFO:Copying training dataset
2025-11-15 23:32:09,877:INFO:Defining folds
2025-11-15 23:32:09,877:INFO:Declaring metric variables
2025-11-15 23:32:09,877:INFO:Importing untrained model
2025-11-15 23:32:09,877:INFO:Gradient Boosting Classifier Imported successfully
2025-11-15 23:32:09,877:INFO:Starting cross validation
2025-11-15 23:32:09,879:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:21,311:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:21,314:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:21,317:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:21,325:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:21,344:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:21,359:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:21,364:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:21,368:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:21,376:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:21,396:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:21,406:INFO:Calculating mean and std
2025-11-15 23:32:21,407:INFO:Creating metrics dataframe
2025-11-15 23:32:21,408:INFO:Uploading results into container
2025-11-15 23:32:21,409:INFO:Uploading model into container now
2025-11-15 23:32:21,409:INFO:_master_model_container: 10
2025-11-15 23:32:21,409:INFO:_display_container: 2
2025-11-15 23:32:21,409:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-11-15 23:32:21,409:INFO:create_model() successfully completed......................................
2025-11-15 23:32:21,496:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:21,497:INFO:Creating metrics dataframe
2025-11-15 23:32:21,500:INFO:Initializing Linear Discriminant Analysis
2025-11-15 23:32:21,500:INFO:Total runtime is 0.5057101368904114 minutes
2025-11-15 23:32:21,500:INFO:SubProcess create_model() called ==================================
2025-11-15 23:32:21,500:INFO:Initializing create_model()
2025-11-15 23:32:21,500:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:21,500:INFO:Checking exceptions
2025-11-15 23:32:21,500:INFO:Importing libraries
2025-11-15 23:32:21,500:INFO:Copying training dataset
2025-11-15 23:32:21,511:INFO:Defining folds
2025-11-15 23:32:21,511:INFO:Declaring metric variables
2025-11-15 23:32:21,511:INFO:Importing untrained model
2025-11-15 23:32:21,512:INFO:Linear Discriminant Analysis Imported successfully
2025-11-15 23:32:21,512:INFO:Starting cross validation
2025-11-15 23:32:21,513:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:22,111:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:22,114:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:22,115:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:22,115:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:22,118:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:22,120:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:22,132:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:22,138:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:22,140:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:22,157:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-11-15 23:32:22,172:INFO:Calculating mean and std
2025-11-15 23:32:22,173:INFO:Creating metrics dataframe
2025-11-15 23:32:22,175:INFO:Uploading results into container
2025-11-15 23:32:22,175:INFO:Uploading model into container now
2025-11-15 23:32:22,176:INFO:_master_model_container: 11
2025-11-15 23:32:22,176:INFO:_display_container: 2
2025-11-15 23:32:22,176:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-11-15 23:32:22,176:INFO:create_model() successfully completed......................................
2025-11-15 23:32:22,274:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:22,274:INFO:Creating metrics dataframe
2025-11-15 23:32:22,277:INFO:Initializing Extra Trees Classifier
2025-11-15 23:32:22,277:INFO:Total runtime is 0.5186499396959942 minutes
2025-11-15 23:32:22,277:INFO:SubProcess create_model() called ==================================
2025-11-15 23:32:22,277:INFO:Initializing create_model()
2025-11-15 23:32:22,277:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:22,277:INFO:Checking exceptions
2025-11-15 23:32:22,277:INFO:Importing libraries
2025-11-15 23:32:22,277:INFO:Copying training dataset
2025-11-15 23:32:22,287:INFO:Defining folds
2025-11-15 23:32:22,287:INFO:Declaring metric variables
2025-11-15 23:32:22,287:INFO:Importing untrained model
2025-11-15 23:32:22,288:INFO:Extra Trees Classifier Imported successfully
2025-11-15 23:32:22,288:INFO:Starting cross validation
2025-11-15 23:32:22,289:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:23,665:INFO:Calculating mean and std
2025-11-15 23:32:23,665:INFO:Creating metrics dataframe
2025-11-15 23:32:23,667:INFO:Uploading results into container
2025-11-15 23:32:23,667:INFO:Uploading model into container now
2025-11-15 23:32:23,668:INFO:_master_model_container: 12
2025-11-15 23:32:23,668:INFO:_display_container: 2
2025-11-15 23:32:23,668:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2025-11-15 23:32:23,668:INFO:create_model() successfully completed......................................
2025-11-15 23:32:23,764:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:23,764:INFO:Creating metrics dataframe
2025-11-15 23:32:23,768:INFO:Initializing Extreme Gradient Boosting
2025-11-15 23:32:23,768:INFO:Total runtime is 0.543510921796163 minutes
2025-11-15 23:32:23,768:INFO:SubProcess create_model() called ==================================
2025-11-15 23:32:23,768:INFO:Initializing create_model()
2025-11-15 23:32:23,768:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=xgboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:23,768:INFO:Checking exceptions
2025-11-15 23:32:23,768:INFO:Importing libraries
2025-11-15 23:32:23,768:INFO:Copying training dataset
2025-11-15 23:32:23,781:INFO:Defining folds
2025-11-15 23:32:23,781:INFO:Declaring metric variables
2025-11-15 23:32:23,781:INFO:Importing untrained model
2025-11-15 23:32:23,782:INFO:Extreme Gradient Boosting Imported successfully
2025-11-15 23:32:23,783:INFO:Starting cross validation
2025-11-15 23:32:23,784:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:30,093:INFO:Calculating mean and std
2025-11-15 23:32:30,093:INFO:Creating metrics dataframe
2025-11-15 23:32:30,096:INFO:Uploading results into container
2025-11-15 23:32:30,096:INFO:Uploading model into container now
2025-11-15 23:32:30,096:INFO:_master_model_container: 13
2025-11-15 23:32:30,097:INFO:_display_container: 2
2025-11-15 23:32:30,097:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2025-11-15 23:32:30,097:INFO:create_model() successfully completed......................................
2025-11-15 23:32:30,195:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:30,196:INFO:Creating metrics dataframe
2025-11-15 23:32:30,198:INFO:Initializing Light Gradient Boosting Machine
2025-11-15 23:32:30,198:INFO:Total runtime is 0.6506738662719728 minutes
2025-11-15 23:32:30,198:INFO:SubProcess create_model() called ==================================
2025-11-15 23:32:30,198:INFO:Initializing create_model()
2025-11-15 23:32:30,198:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:30,198:INFO:Checking exceptions
2025-11-15 23:32:30,198:INFO:Importing libraries
2025-11-15 23:32:30,198:INFO:Copying training dataset
2025-11-15 23:32:30,209:INFO:Defining folds
2025-11-15 23:32:30,209:INFO:Declaring metric variables
2025-11-15 23:32:30,209:INFO:Importing untrained model
2025-11-15 23:32:30,209:INFO:Light Gradient Boosting Machine Imported successfully
2025-11-15 23:32:30,209:INFO:Starting cross validation
2025-11-15 23:32:30,210:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:37,549:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-15 23:32:37,556:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-15 23:32:37,556:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-15 23:32:37,606:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-15 23:32:37,608:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-15 23:32:37,624:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-15 23:32:37,625:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-15 23:32:37,633:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-15 23:32:37,634:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-15 23:32:37,646:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 345, in _score
    y_pred = method_caller(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_scorer.py", line 87, in _cached_call
    result, _ = _get_response_values(
  File "C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\utils\_response.py", line 231, in _get_response_values
    raise ValueError(
ValueError: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.

  warnings.warn(

2025-11-15 23:32:37,673:INFO:Calculating mean and std
2025-11-15 23:32:37,674:INFO:Creating metrics dataframe
2025-11-15 23:32:37,676:INFO:Uploading results into container
2025-11-15 23:32:37,677:INFO:Uploading model into container now
2025-11-15 23:32:37,678:INFO:_master_model_container: 14
2025-11-15 23:32:37,678:INFO:_display_container: 2
2025-11-15 23:32:37,678:INFO:<lightgbm.LGBMClassifier object at 0x000002757E209460>
2025-11-15 23:32:37,678:INFO:create_model() successfully completed......................................
2025-11-15 23:32:37,769:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:37,769:INFO:Creating metrics dataframe
2025-11-15 23:32:37,771:INFO:Initializing Dummy Classifier
2025-11-15 23:32:37,771:INFO:Total runtime is 0.7768889983495078 minutes
2025-11-15 23:32:37,771:INFO:SubProcess create_model() called ==================================
2025-11-15 23:32:37,772:INFO:Initializing create_model()
2025-11-15 23:32:37,772:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002757E174940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:37,772:INFO:Checking exceptions
2025-11-15 23:32:37,772:INFO:Importing libraries
2025-11-15 23:32:37,772:INFO:Copying training dataset
2025-11-15 23:32:37,782:INFO:Defining folds
2025-11-15 23:32:37,782:INFO:Declaring metric variables
2025-11-15 23:32:37,783:INFO:Importing untrained model
2025-11-15 23:32:37,783:INFO:Dummy Classifier Imported successfully
2025-11-15 23:32:37,783:INFO:Starting cross validation
2025-11-15 23:32:37,784:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-11-15 23:32:38,468:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-15 23:32:38,476:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-15 23:32:38,479:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-15 23:32:38,486:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-15 23:32:38,489:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-15 23:32:38,491:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-15 23:32:38,492:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-15 23:32:38,496:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-15 23:32:38,497:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-15 23:32:38,518:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-11-15 23:32:38,527:INFO:Calculating mean and std
2025-11-15 23:32:38,528:INFO:Creating metrics dataframe
2025-11-15 23:32:38,530:INFO:Uploading results into container
2025-11-15 23:32:38,531:INFO:Uploading model into container now
2025-11-15 23:32:38,531:INFO:_master_model_container: 15
2025-11-15 23:32:38,531:INFO:_display_container: 2
2025-11-15 23:32:38,531:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2025-11-15 23:32:38,531:INFO:create_model() successfully completed......................................
2025-11-15 23:32:38,613:INFO:SubProcess create_model() end ==================================
2025-11-15 23:32:38,614:INFO:Creating metrics dataframe
2025-11-15 23:32:38,646:WARNING:C:\Users\maraw\.vscode\Vibration-TCM-Deep-Learning-Pipeline\.venv\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-11-15 23:32:38,649:INFO:Initializing create_model()
2025-11-15 23:32:38,649:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:38,649:INFO:Checking exceptions
2025-11-15 23:32:38,649:INFO:Importing libraries
2025-11-15 23:32:38,649:INFO:Copying training dataset
2025-11-15 23:32:38,661:INFO:Defining folds
2025-11-15 23:32:38,661:INFO:Declaring metric variables
2025-11-15 23:32:38,661:INFO:Importing untrained model
2025-11-15 23:32:38,661:INFO:Declaring custom model
2025-11-15 23:32:38,661:INFO:Gradient Boosting Classifier Imported successfully
2025-11-15 23:32:38,662:INFO:Cross validation set to False
2025-11-15 23:32:38,662:INFO:Fitting Model
2025-11-15 23:32:45,747:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-11-15 23:32:45,747:INFO:create_model() successfully completed......................................
2025-11-15 23:32:45,833:INFO:_master_model_container: 15
2025-11-15 23:32:45,833:INFO:_display_container: 2
2025-11-15 23:32:45,834:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-11-15 23:32:45,834:INFO:compare_models() successfully completed......................................
2025-11-15 23:32:45,835:INFO:Initializing finalize_model()
2025-11-15 23:32:45,835:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-11-15 23:32:45,835:INFO:Finalizing GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-11-15 23:32:45,847:INFO:Initializing create_model()
2025-11-15 23:32:45,847:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-11-15 23:32:45,847:INFO:Checking exceptions
2025-11-15 23:32:45,848:INFO:Importing libraries
2025-11-15 23:32:45,848:INFO:Copying training dataset
2025-11-15 23:32:45,849:INFO:Defining folds
2025-11-15 23:32:45,849:INFO:Declaring metric variables
2025-11-15 23:32:45,849:INFO:Importing untrained model
2025-11-15 23:32:45,849:INFO:Declaring custom model
2025-11-15 23:32:45,850:INFO:Gradient Boosting Classifier Imported successfully
2025-11-15 23:32:45,851:INFO:Cross validation set to False
2025-11-15 23:32:45,851:INFO:Fitting Model
2025-11-15 23:32:54,229:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2025-11-15 23:32:54,229:INFO:create_model() successfully completed......................................
2025-11-15 23:32:54,309:INFO:_master_model_container: 15
2025-11-15 23:32:54,309:INFO:_display_container: 2
2025-11-15 23:32:54,313:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2025-11-15 23:32:54,314:INFO:finalize_model() successfully completed......................................
2025-11-15 23:32:54,393:INFO:Initializing predict_model()
2025-11-15 23:32:54,393:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002757DB92430>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002757E2033A0>)
2025-11-15 23:32:54,393:INFO:Checking exceptions
2025-11-15 23:32:54,393:INFO:Preloading libraries
2025-11-15 23:32:54,619:INFO:Initializing save_model()
2025-11-15 23:32:54,619:INFO:save_model(model=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False), model_name=reports\phase3\pycaret\best_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\maraw\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_fa...
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('transformation',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=PowerTransformer(copy=True,
                                                                 method='yeo-johnson',
                                                                 standardize=False))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-11-15 23:32:54,619:INFO:Adding model into prep_pipe
2025-11-15 23:32:54,619:WARNING:Only Model saved as it was a pipeline.
2025-11-15 23:32:54,635:INFO:reports\phase3\pycaret\best_model.pkl saved in current working directory
2025-11-15 23:32:54,641:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['acceleration_xg_time_mean',
                                             'acceleration_xg_time_std',
                                             'acceleration_xg_time_rms',
                                             'acceleration_xg_time_mad',
                                             'acceleration_xg_time_skewness',
                                             'acceleration_xg_time_kurtosis',
                                             'acceleration_xg_time_crest_factor',
                                             'acceleration_xg_freq_spectral_ene...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=42, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2025-11-15 23:32:54,641:INFO:save_model() successfully completed......................................
