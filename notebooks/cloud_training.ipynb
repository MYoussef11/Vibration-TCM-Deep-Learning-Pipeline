{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Vibration TCM - Deep Learning Pipeline (Cloud Training)\n",
                "\n",
                "This notebook allows you to train the **Transfer Learning (EfficientNet)** model using free GPU resources on Google Colab or Kaggle.\n",
                "\n",
                "## Instructions\n",
                "1.  **Upload Data**: Upload your `phase3_datasets.npz` file to the notebook environment (drag & drop to the file browser on the left).\n",
                "2.  **Enable GPU**: Go to `Runtime` > `Change runtime type` > Select `T4 GPU` (or any available GPU).\n",
                "3.  **Run All**: Click `Runtime` > `Run all`.\n",
                "4.  **Download Results**: After training, download the `best_model.h5` and `study_results.json` files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install optuna tensorflow scikit-learn pandas numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "import optuna\n",
                "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
                "import json\n",
                "import os\n",
                "\n",
                "# Check GPU\n",
                "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
                "if len(tf.config.list_physical_devices('GPU')) == 0:\n",
                "    print(\"WARNING: No GPU detected. Training will be slow!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATASET_PATH = \"phase3_datasets.npz\"  # Upload this file!\n",
                "\n",
                "def load_dataset(npz_path):\n",
                "    if not os.path.exists(npz_path):\n",
                "        raise FileNotFoundError(f\"Dataset not found: {npz_path}. Please upload it!\")\n",
                "    data = np.load(npz_path, allow_pickle=True)\n",
                "    return {key: data[key] for key in data.files}\n",
                "\n",
                "def split_train_val(train_indices, groups, val_size, seed):\n",
                "    splitter = GroupShuffleSplit(n_splits=1, test_size=val_size, random_state=seed)\n",
                "    dummy = np.zeros(len(train_indices))\n",
                "    rel_train_idx, rel_val_idx = next(splitter.split(dummy, groups=groups[train_indices]))\n",
                "    return train_indices[rel_train_idx], train_indices[rel_val_idx]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_transfer_learning(input_shape, num_classes, params):\n",
                "    # Input shape is (freq_bins, time_bins, 1)\n",
                "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
                "    \n",
                "    # Convert 1-channel to 3-channel by repeating\n",
                "    x = tf.keras.layers.Conv2D(3, (1, 1), padding='same')(inputs)\n",
                "    \n",
                "    # Resize to a size compatible with EfficientNet (e.g., 224x224)\n",
                "    x = tf.keras.layers.Resizing(224, 224)(x)\n",
                "    \n",
                "    # Load EfficientNetB0 pre-trained on ImageNet\n",
                "    base_model = tf.keras.applications.EfficientNetB0(\n",
                "        include_top=False,\n",
                "        weights=\"imagenet\",\n",
                "        input_shape=(None, None, 3) \n",
                "    )\n",
                "    \n",
                "    # Pass the 3-channel input through the base model\n",
                "    x = base_model(x)\n",
                "    \n",
                "    # Unfreeze top N layers\n",
                "    base_model.trainable = True\n",
                "    fine_tune_at = len(base_model.layers) - params[\"fine_tune_layers\"]\n",
                "    for layer in base_model.layers[:fine_tune_at]:\n",
                "        layer.trainable = False\n",
                "        \n",
                "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
                "    x = tf.keras.layers.Dropout(params[\"dropout\"])(x)\n",
                "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
                "    \n",
                "    return tf.keras.Model(inputs, outputs)\n",
                "\n",
                "def build_cnn2d(input_shape, num_classes, params):\n",
                "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
                "    # Resize to a reasonable square size (e.g., 64x64)\n",
                "    x = tf.keras.layers.Resizing(64, 64)(inputs)\n",
                "    x = tf.keras.layers.Conv2D(params[\"filters1\"], (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
                "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
                "    x = tf.keras.layers.Conv2D(params[\"filters2\"], (3, 3), padding=\"same\", activation=\"relu\")(x)\n",
                "    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
                "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
                "    x = tf.keras.layers.Dropout(params[\"dropout\"])(x)\n",
                "    x = tf.keras.layers.Dense(params[\"dense_units\"], activation=\"relu\")(x)\n",
                "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
                "    return tf.keras.Model(inputs, outputs)\n",
                "\n",
                "def compile_model(model, lr):\n",
                "    model.compile(\n",
                "        optimizer=tf.keras.optimizers.Adam(lr),\n",
                "        loss=\"sparse_categorical_crossentropy\",\n",
                "        metrics=[\"accuracy\"],\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def objective(trial):\n",
                "    tf.keras.backend.clear_session()\n",
                "    \n",
                "    # Choose model type (optional, or just stick to one)\n",
                "    model_type = trial.suggest_categorical(\"model_type\", [\"transfer_learning\", \"cnn2d\"])\n",
                "    \n",
                "    if model_type == \"transfer_learning\":\n",
                "        params = {\n",
                "            \"fine_tune_layers\": trial.suggest_int(\"fine_tune_layers\", 0, 50),\n",
                "            \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n",
                "        }\n",
                "        build_fn = lambda: build_transfer_learning(input_spec_shape, num_classes, params)\n",
                "    else:\n",
                "        params = {\n",
                "            \"filters1\": trial.suggest_categorical(\"filters1\", [16, 32, 64]),\n",
                "            \"filters2\": trial.suggest_categorical(\"filters2\", [32, 64, 128]),\n",
                "            \"dropout\": trial.suggest_float(\"dropout\", 0.1, 0.5),\n",
                "            \"dense_units\": trial.suggest_categorical(\"dense_units\", [64, 128]),\n",
                "        }\n",
                "        build_fn = lambda: build_cnn2d(input_spec_shape, num_classes, params)\n",
                "\n",
                "    lr = trial.suggest_float(\"learning_rate\", 1e-4, 5e-3, log=True)\n",
                "    \n",
                "    input_spec_shape = X_spec.shape[1:]\n",
                "    num_classes = len(np.unique(y))\n",
                "    \n",
                "    accuracies = []\n",
                "    # OPTIMIZATION: Use 3 folds instead of 5 to save GPU time\n",
                "    gkf = GroupKFold(n_splits=3) \n",
                "    \n",
                "    for fold_idx, (train_idx_base, test_idx) in enumerate(gkf.split(X_spec, y, groups)):\n",
                "        train_idx, val_idx = split_train_val(train_idx_base, groups, 0.15, 42 + fold_idx)\n",
                "        \n",
                "        model = build_fn()\n",
                "        compile_model(model, lr)\n",
                "        \n",
                "        callbacks = [\n",
                "            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
                "        ]\n",
                "        \n",
                "        model.fit(\n",
                "            X_spec[train_idx],\n",
                "            y[train_idx],\n",
                "            validation_data=(X_spec[val_idx], y[val_idx]),\n",
                "            epochs=12, # Reduced epochs (was 15/40)\n",
                "            batch_size=32,\n",
                "            callbacks=callbacks,\n",
                "            verbose=1\n",
                "        )\n",
                "        \n",
                "        _, test_acc = model.evaluate(X_spec[test_idx], y[test_idx], verbose=0)\n",
                "        accuracies.append(test_acc)\n",
                "        \n",
                "    return np.mean(accuracies)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Main Execution\n",
                "try:\n",
                "    dataset = load_dataset(DATASET_PATH)\n",
                "    X_spec = dataset[\"X_spec\"]\n",
                "    y = dataset[\"y\"]\n",
                "    groups = dataset[\"groups\"]\n",
                "    \n",
                "    print(f\"Loaded dataset with {len(y)} samples.\")\n",
                "    \n",
                "    study = optuna.create_study(direction=\"maximize\")\n",
                "    # Run fewer trials to respect GPU limits\n",
                "    study.optimize(objective, n_trials=10)\n",
                "    \n",
                "    print(\"Best trial:\")\n",
                "    trial = study.best_trial\n",
                "    print(\"  Value: \", trial.value)\n",
                "    print(\"  Params: \")\n",
                "    for key, value in trial.params.items():\n",
                "        print(f\"    {key}: {value}\")\n",
                "        \n",
                "    # Save best params\n",
                "    with open(\"study_results.json\", \"w\") as f:\n",
                "        json.dump(trial.params, f)\n",
                "        \n",
                "except FileNotFoundError as e:\n",
                "    print(e)\n",
                "except Exception as e:\n",
                "    print(f\"An error occurred: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}